{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300bcc4-eacb-4b6c-b4c3-6de9421eb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 1 - Modelling for Uni modal case (Pred Z and PDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771fd55-11a4-43ff-8246-5b2840c95480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53fa9e-6aaf-498a-a7ae-6fc7dc03cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c79ff-329b-4969-9028-52e81bd7f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedbf2d-d5bd-42d2-82af-d76edd7276e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d911ab0-440a-4ac0-bebd-053e97b7340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "print(data[\" PDI\"].min())\n",
    "print(data[\" PDI\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eacd02-c635-4b89-9459-85261a83586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(math.log10((((10 ** -1.632774717) + 1)*1))) # log(PDI - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ba321-422c-4315-9aa1-fb2a1affaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "print(data[\" PDI\"].min())\n",
    "print(data[\" PDI\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ba494-697d-4755-a9d2-cccfa89ce5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(10**(3) - 10**(0.477121255))*0.15\n",
    "# print(data[\"Z\"].min())\n",
    "# print(data[\"Z\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47b23d-e913-4a60-9649-ab548dc06129",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.iloc[:,97:]\n",
    "print(d.head(5))\n",
    "d = d.values.flatten()\n",
    "d\n",
    "print(d.min())\n",
    "print(d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50592197-b842-4f3f-8f82-6f061ee0be56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in data.keys():\n",
    "#     print(i + \" : \"  + str(data[i].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc88640-0af8-481a-bb4b-032e0d453e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24763d-de66-4d04-b7bf-98f422fd9b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lis = []\n",
    "for i in ['Z']:\n",
    "  lis.append(y[i].unique())\n",
    "\n",
    "lis = [item for sublist in lis for item in sublist]\n",
    "for P in lis:\n",
    "    c=0\n",
    "    for i in y['Z']:\n",
    "      if i==P:\n",
    "        c = c+1\n",
    "    print(P,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02511c6-2867-4696-9a8e-06ff44d9d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c=0\n",
    "# for i in y[' PDI']:\n",
    "#   if i<0.65 and i>0.4:\n",
    "#     c = c+1\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150e94b-32e9-4fc3-93e4-b019010d2347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y[\" PDI\"])\n",
    "plt.xlabel(\"PDI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of PDI\")\n",
    "plt.show()\n",
    "plt.hist(y[\"Z\"])\n",
    "plt.xlabel(\"Z\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Z\")\n",
    "plt.show()\n",
    "#plt.hist(y[\"Z\"])\n",
    "#plt.show() \n",
    "# plt.hist(y_train[\" PDI\"])\n",
    "# plt.show() \n",
    "# plt.hist(y_test[\" PDI\"])\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e07e28-dcbb-40e2-9cf3-9135bfee94da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a67ac4-d25d-4d93-bdff-c55453b33e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(\" \\n\")\n",
    "print(type(y_trainNP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657efca-8c62-494f-840c-0b989c6a5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee67481-32f2-4492-948a-2da8a8b52419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010d0d9-4c8a-48f5-bdb2-d9d85e7bf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.read_csv('run0.dat', delim_whitespace=True,header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3df738-604c-4f1d-ac9d-3f7ef2a29268",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.log10(freq['#w'])\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd7db-c4f5-4a04-a644-ba1e7e158522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zpercentiles = np.percentile(data[\"Z\"], [25, 50, 75])\n",
    "PDIpercentiles = np.percentile(data[\" PDI\"], [25, 50, 75])\n",
    "print(Zpercentiles)\n",
    "print(PDIpercentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68055e-8ada-4c80-b50f-693329e1503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_nearest(array, values):\n",
    "    \"\"\"Find the nearest value in a NumPy array.\"\"\"\n",
    "    array = np.asarray(array)  # Convert to NumPy array if not already\n",
    "    idx = list()\n",
    "    for i,value in enumerate(values):\n",
    "        idx.append((np.abs(array - value)).argmin())  # Find index of the nearest value\n",
    "    newarr = list()\n",
    "    for val in idx:\n",
    "        newarr.append(array[val])\n",
    "    return array[idx]\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 4, 5.5, 8, 10])\n",
    "# targets = [5,11,12]\n",
    "# nearest_value = find_nearest(data, targets)\n",
    "# print(f\"The nearest value to {target} is {nearest_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3c5d4-d8fa-4159-9152-ff362ad614da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zpercentiles = np.percentile(data[\"Z\"], [0, 25, 50, 75, 100])\n",
    "PDIpercentiles = np.percentile(data[\" PDI\"], [0, 25, 50, 75, 100])\n",
    "# print(Zpercentiles)\n",
    "# print(data[\"Z\"])\n",
    "Zpercentiles = find_nearest(data[\"Z\"], Zpercentiles)\n",
    "PDIpercentiles = find_nearest(data[\" PDI\"], PDIpercentiles)\n",
    "terms = [\"Very Low\",\"Low\",\"Mid\",\"High\",\"Very High\"]\n",
    "for term1,i in enumerate(Zpercentiles):\n",
    "    for term2,j in enumerate(PDIpercentiles):\n",
    "        # needrowGprime = pd.DataFrame()\n",
    "        # needrowGdprime = pd.DataFrame()\n",
    "        needrowGprime = data[data[\"Z\"]==i][data[\" PDI\"]==j].iloc[:, 2:97]\n",
    "        needrowGdprime = data[data[\"Z\"]==i][data[\" PDI\"]==j].iloc[:, 97:]\n",
    "        # print(data[data[\"Z\"]==i].iloc[:5, :7])\n",
    "        # print(data[data[\" PDI\"]==j].iloc[:5, :7])\n",
    "        #Z errors wrt PDI\n",
    "        plt.plot(freq, needrowGprime.iloc[0].values,marker='o', linestyle='-', color='red',label=f\"G' curve with {terms[term1]} Z and {terms[term2]} PDI\")\n",
    "        plt.plot(freq, needrowGdprime.iloc[0].values,marker='o', linestyle='-', color='b',label=f\"G'' curve with {terms[term1]} Z and {terms[term2]} PDI\")\n",
    "        plt.xlabel(r'$\\overline{\\omega}$')\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r\"$\\overline{G'} \\text{ and } \\overline{G''}$\")\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(f\"Flow Curve at {terms[term1]} $\\\\overline{{Z}}$ and {terms[term2]} $\\\\overline{{PDI}}$\")\n",
    "        plt.text(min(freq), max(max(needrowGprime.iloc[0].values), max(needrowGdprime.iloc[0].values))-1, f\"$\\\\overline{{Z}} = {round(Zpercentiles[term1],2)}$  , $\\\\overline{{PDI}} = {round(PDIpercentiles[term2],2)}$  \", fontsize=10, color='black')\n",
    "        # Save the plot to a file\n",
    "        plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\graphs\\\\graph_{0}{1}.png'.format(term1,term2), format='png',dpi=600)  # Save as PNG file\n",
    "        plt.show()\n",
    "        # print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3f4dc-e143-4291-a133-d1429dd3ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Create a figure with subplots arranged in a grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "\n",
    "# Load and display each image in the grid\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\graphs\\\\graph_{0}{1}.png'.format(abs(4 - i),j)  # Path to the image file\n",
    "        img = mpimg.imread(img_path)  # Load the image\n",
    "        axs[i, j].imshow(img)  # Display the image\n",
    "        axs[i, j].axis('off')  # Hide the axis\n",
    "\n",
    "# Add overall x-axis and y-axis labels\n",
    "fig.supxlabel(r'$\\overline{PDI} \\longrightarrow$', fontsize=15)\n",
    "fig.supylabel(r'$\\overline{Z} \\longrightarrow$', fontsize=15)\n",
    "fig.suptitle(f'Flow curves at different values of $\\\\overline{{Z}}$ and $\\\\overline{{PDI}}$', fontsize=15)\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()  # Adjust bottom and top margins #rect=[0, 0, 1, 0.96]\n",
    "# Adjust layout to prevent overlap and increase spacing\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)  # Adjust space between subplots\n",
    "xspace = 0.06\n",
    "yspace = 0.04\n",
    "space = 0.0001\n",
    "plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "# Adding a note using fig.text\n",
    "fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\graphs\\\\allgraph.png', format='png',dpi=300)  # Save as PNG file\n",
    "plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4b1fa-b5e5-47fb-9b24-0b8eea778cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a601c-382f-485c-809f-712faddc8ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b59c5-bec8-4e9f-aa3c-ae4decf0d99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40aab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(64, input_shape=(1,190))) #return_sequences=True\n",
    "    #model.add(LSTM(16))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81070c88-fb39-43f0-af20-ee111c649adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #metrics=['mae']\n",
    "# #Define the MLP model\n",
    "# def create_mlp_model(input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#     #model.add(Dropout(0.25))\n",
    "#     # model.add(Dense(128, activation='relu'))\n",
    "#     # model.add(Dropout(0.15))\n",
    "#     model.add(Dense(16, activation='relu'))\n",
    "#     model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "#     return model\n",
    "\n",
    "# # Create the model\n",
    "# model = create_mlp_model(X_train.shape[1])\n",
    "\n",
    "# #Create the Adam optimizer with a custom learning rate\n",
    "# learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "# custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=custom_optimizer, loss='mean_squared_error')#metrics=['mae']\n",
    "\n",
    "# # Define early stopping to prevent overfitting\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# type(X_train)\n",
    "# type(y_train)\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_trainNP, \n",
    "#                     validation_split=0.2, \n",
    "#                     epochs=100, \n",
    "#                     batch_size=32, \n",
    "#                     callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# #Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e5e43-f292-468f-b65f-d04a268c9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 3.094820931437425e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5f535-fb93-47fd-b71b-f41417dca034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d1eef-3777-4673-9318-650978fa2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aaad7a-8f73-4d98-8c72-317f749985ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8fd15-3be0-4168-83ae-f9d1bbe69947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a407c3-8764-4911-95fa-a2e1ad73ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) = 0.0035236082972914915\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384efd4e-e043-48c3-bcde-addc6df46ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = 0.006535302287162144\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9d61d-ccc4-406f-a306-9eca1101a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6bd52-bbcc-487b-87f6-414bc074b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 2 - Error Analysis of Unimodal case (Z and PDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821379b-a091-41ce-b101-86fe11cd035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8f5e4-60ab-4b0d-b1f5-72842941da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "display(Math(r'\\overline{Z} \\text{ Errors vs } \\overline{Z}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a042a-4da8-4f6c-80a3-7f49364351f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fdaf1-f415-4877-b9da-04c73e136d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dataset\n",
    "# data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "#data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c4c26-74a7-46ed-a821-08b9380e776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "# y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# xcopytest = X_test\n",
    "\n",
    "# # Standardize the data (mean=0, variance=1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# #X_trainNP = X_train.flatten()\n",
    "# y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309874b5-a729-4fca-92d8-ff6b037d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# loaded_model = tf.keras.models.load_model(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model.h5\")\n",
    "\n",
    "# X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# # Make predictions with the loaded model\n",
    "# y_pred_loaded = loaded_model.predict(X_test_reshaped)\n",
    "\n",
    "# df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "# df1 = df1.reset_index(drop=False)\n",
    "# df1[\"index\"] = df1[\"index\"] + 2\n",
    "# df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "# df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "# df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b2281-44a8-4ab5-af7d-002b58a0e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e0b97-a9a2-46be-beea-2535a7591e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96081491-bf6b-48b7-a3d2-2f6472e734ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) = 0.0035236082972914915\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c246267-dd93-4a9a-ac12-33c16001464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = 0.006535302287162144\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175314d0-86e8-4613-8126-ec6edabc4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Highest Z error : {max(df['zerrors'])}\")\n",
    "print(f\"Highest PDI error : {max(df['pdierrors'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6edaa-6525-41b7-8113-e49776fee886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"zerrorsreal\"] = (10**df[\"Z\"] - 10**df[\"ZPRED\"]).abs()\n",
    "# sorted_df = df.sort_values(by=\"zerrorsreal\", ascending=False)\n",
    "# print(\"Z STUFF\")\n",
    "# print(sorted_df[\"zerrorsreal\"].iloc[0])\n",
    "# print(\"Highest error Actual : \",sorted_df[\"Z\"].iloc[0])\n",
    "# print(\"Highest error Pred : \",sorted_df[\"ZPRED\"].iloc[0])\n",
    "# print(\"Error : \",sorted_df[\"ZPRED\"].iloc[0] - sorted_df[\"Z\"].iloc[0])\n",
    "# df[\"pdierrorsreal\"] = (10**df[\" PDI\"] - 10**df[\" PDIPRED\"]).abs()\n",
    "# sorted_df = df.sort_values(by=\"pdierrorsreal\", ascending=False)\n",
    "# print(\"PDI STUFF\")\n",
    "# print(sorted_df[\"pdierrorsreal\"].iloc[0])\n",
    "# print(\"Highest error Actual : \",sorted_df[\" PDI\"].iloc[0])\n",
    "# print(\"Highest error Pred : \",sorted_df[\" PDIPRED\"].iloc[0])\n",
    "# print(\"Error : \",sorted_df[\" PDIPRED\"].iloc[0] - sorted_df[\" PDI\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d979e-53fe-40f8-b60a-b57e61174f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "def code1(y1,y2,y3):\n",
    "    strings = [\"Z\"]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s])\n",
    "        points = [val]\n",
    "        while val + 0.125 < max(df[s]):\n",
    "            val = val + 0.125\n",
    "            points.append(val)\n",
    "        points.append(max(df[s]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s]>=points[i]][df[s]<points[i+1]][s.lower()+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s]>=points[i]][df[s]<=points[i+1]][s.lower()+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        newdataZ = newdata\n",
    "        pointsZ = [x + 0.0625 for x in points[:-1]]\n",
    "        plt.plot(pointsZ, newdataZ,marker='o', linestyle='-', color='lime')\n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{Z}$')\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(7.5, 6))\n",
    "        #plt.figure(figsize=(5, 4))\n",
    "        \n",
    "        median_x = np.median(df[\"Z\"])\n",
    "        median_y = np.median(df[\"zerrors\"])\n",
    "        \n",
    "        Q1_x, Q3_x = np.percentile(df[\"Z\"], [25, 75])\n",
    "        Q1_y, Q3_y = np.percentile(df[\"zerrors\"], [25, 75])\n",
    "        \n",
    "        IQR_x = Q3_x - Q1_x\n",
    "        IQR_y = Q3_y - Q1_y\n",
    "        \n",
    "        # Plot the density heatmap\n",
    "        sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "        \n",
    "        # plotting the points \n",
    "        plt.scatter(df[\"Z\"], df[\"zerrors\"],s=0.1, c='black', alpha=0.5)\n",
    "        \n",
    "        plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "        # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "        plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "        plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "        # plt.fill_betweenx([Q1_y, Q3_y], df[\"Z\"].min(), df[\"Z\"].max(), color='gray', alpha=0.2, label='IQR Y')\n",
    "        # plt.fill_betweenx([Q1_x, Q3_x], df[\"zerrors\"].min(), df[\"zerrors\"].max(), color='lightblue', alpha=0.2, label='IQR X')\n",
    "        \n",
    "        # Plot the density heatmap\n",
    "        #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "        plt.plot(pointsZ, newdataZ,marker='o', linestyle='-', color='lime')\n",
    "        \n",
    "        # Set x and y limits\n",
    "        plt.xlim(df[\"Z\"].min()-0.3, df[\"Z\"].max()+0.3)  # Set the x-axis limit from 0 to 1\n",
    "        plt.ylim(0, y1)  # Set the y-axis limit from 0 to 1\n",
    "        \n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{Z}$')\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{Z} \\text{ Errors with Density Heatmap}$')\n",
    "         \n",
    "        # function to show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    strings = [\" PDI\"]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s])\n",
    "        points = [val]\n",
    "        while val + 0.125 < max(df[s]):\n",
    "            val = val + 0.125\n",
    "            points.append(val)\n",
    "        points.append(max(df[s]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s]>=points[i]][df[s]<points[i+1]][\"pdi\"+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s]>=points[i]][df[s]<=points[i+1]][\"pdi\"+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        newdataPDI = newdata\n",
    "        pointsPDI = [x + 0.0625 for x in points[:-1]]\n",
    "        plt.plot(pointsPDI, newdataPDI,marker='o', linestyle='-', color='lime')\n",
    "        # naming the x axis\n",
    "        # naming the x axis\n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{PDI}$')\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(7.5, 6))\n",
    "        #plt.figure(figsize=(5, 4))\n",
    "        median_x = np.median(df[\" PDI\"])\n",
    "        median_y = np.median(df[\"pdierrors\"])\n",
    "        \n",
    "        Q1_x, Q3_x = np.percentile(df[\" PDI\"], [25, 75])\n",
    "        Q1_y, Q3_y = np.percentile(df[\"pdierrors\"], [25, 75])\n",
    "        \n",
    "        IQR_x = Q3_x - Q1_x\n",
    "        IQR_y = Q3_y - Q1_y\n",
    "        # Plot the density heatmap\n",
    "        sns.kdeplot(x=df[\" PDI\"], y=df[\"pdierrors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "        \n",
    "        # plotting the points \n",
    "        plt.scatter(df[\" PDI\"], df[\"pdierrors\"],s=0.1, c='black', alpha=0.5)\n",
    "        \n",
    "        plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "        # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "        plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "        plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "        # Plot the density heatmap\n",
    "        #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "        plt.plot(pointsPDI, newdataPDI,marker='o', linestyle='-', color='lime')\n",
    "        plt.xlim(df[\" PDI\"].min()-0.3, df[\" PDI\"].max()+0.3)  # Set the y-axis limit from 0 to 1\n",
    "        plt.ylim(0, y2)  # Set the y-axis limit from 0 to 1\n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{PDI}$')\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{PDI} \\text{ Errors with Density Heatmap}$')\n",
    "         \n",
    "        # function to show the plot\n",
    "        plt.show()\n",
    "\n",
    "    strings = [[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "\n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.125 < max(df[s[0]]):\n",
    "            val = val + 0.125\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{{{0}}}$'.format(s[0]))\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(s[1].upper()))\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(s[0], s[1].upper()))\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(7.5, 6))\n",
    "        #plt.figure(figsize=(5, 4))\n",
    "        \n",
    "        median_x = np.median(df[s[0]])\n",
    "        median_y = np.median(df[s[1]+\"errors\"])\n",
    "        \n",
    "        Q1_x, Q3_x = np.percentile(df[s[0]], [25, 75])\n",
    "        Q1_y, Q3_y = np.percentile(df[s[1]+\"errors\"], [25, 75])\n",
    "        \n",
    "        IQR_x = Q3_x - Q1_x\n",
    "        IQR_y = Q3_y - Q1_y\n",
    "        \n",
    "        # Plot the density heatmap\n",
    "        sns.kdeplot(x=df[s[0]], y=df[s[1]+\"errors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "        \n",
    "        # plotting the points \n",
    "        plt.scatter(df[s[0]], df[s[1]+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "        \n",
    "        plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "        # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "        plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "        plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "        # plt.fill_betweenx([Q1_y, Q3_y], df[\"Z\"].min(), df[\"Z\"].max(), color='gray', alpha=0.2, label='IQR Y')\n",
    "        # plt.fill_betweenx([Q1_x, Q3_x], df[\"zerrors\"].min(), df[\"zerrors\"].max(), color='lightblue', alpha=0.2, label='IQR X')\n",
    "        \n",
    "        # Plot the density heatmap\n",
    "        #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "        plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        \n",
    "        # Set x and y limits\n",
    "        plt.xlim(df[s[0]].min()-0.3, df[s[0]].max()+0.3)  # Set the x-axis limit from 0 to 1\n",
    "        plt.ylim(0, y3)  # Set the y-axis limit from 0 to 1\n",
    "        \n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{{{0}}}$'.format(s[0]))\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(s[1].upper()))\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{{{0}}} \\text{{ Errors with Density Heatmap}}$'.format(s[1].upper()))\n",
    "         \n",
    "        # function to show the plot\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2eff5b-a5f7-4d14-9ec7-3c5c62850c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "code1(0.02,0.06,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11298098-d39f-44a8-b7cd-aa7c72c5644a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b6795-bcec-47ca-8bb6-40b2c908211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 3 - Changing the range of G dash values available for modelling (Unimodal case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337485d-1e33-4d2d-89ba-907ca28a875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12794ed-efe5-4fa3-a651-5f44a650cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1325e-ba2d-4588-b7bc-f87838c75eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eab14b-da07-4c1b-ba5c-1dc69b6f1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = data.iloc[:, 16:56]  # (G'(14) to G'(53)) (10^-3 to 10^3) (40 columns)\n",
    "# X2 = data.iloc[:, 111:151]  # (G''(14) to G''(53)) (10^-3 to 10^3) (40 columns)\n",
    "X1 = data.iloc[:, 22:49]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = data.iloc[:, 117:144]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3c7dd-ceb9-425b-bf77-1a610fae5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed402b-bf9f-4508-9acb-46f0a23dfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(64, input_shape=(1,54))) #return_sequences=True\n",
    "    #model.add(LSTM(16))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd5666-0b1f-4db5-936b-febce4d47092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba75a52-4a08-448e-b0f0-643e79e4746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4e575-85d4-4dcf-af9c-dd5325f626f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3b587-bd90-4958-962b-55a023a79ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b194d-6d76-4cfd-87a7-60b3f8f6b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c1afc-e0b3-43b0-a1a4-1c9d751e9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533be401-3ee8-4996-914e-db03cc7127ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b200e-acc6-47df-8ce6-fb2e107035eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312dea7-88be-4f55-bca2-af92c3898806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 4 - Error Analysis on Restricted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cbb7e-aecb-4cd1-8854-4c853fbb86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4277d-2a37-426f-a79c-a81961821686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea2166-d90b-49b8-9704-0a17da5aebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Load the dataset\n",
    "#data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "#data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356ee31-7f07-4983-8ede-a637003846b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = data.iloc[:, 22:49]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "# X2 = data.iloc[:, 117:144]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "# X = pd.concat([X1, X2], axis=1)\n",
    "# y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# xcopytest = X_test\n",
    "\n",
    "# # Standardize the data (mean=0, variance=1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# #X_trainNP = X_train.flatten()\n",
    "# y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655cb9f-7e81-4e5c-8c51-daebb2120b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# loaded_model = tf.keras.models.load_model(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted.h5\")\n",
    "\n",
    "# X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "# # Make predictions with the loaded model\n",
    "# y_pred_loaded = loaded_model.predict(X_test_reshaped)\n",
    "\n",
    "# df1 = pd.DataFrame(xcopytest,columns = X.columns)\n",
    "# df1 = df1.reset_index(drop=False)\n",
    "# df1[\"index\"] = df1[\"index\"] + 2\n",
    "# df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "# df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "# df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f5954-b97f-4f05-a4d3-d1fea2192280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) = 0.0035236082972914915\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2e548-30c0-434e-99f0-5ca641bcef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = 0.006535302287162144\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d16c89-c092-4af7-bc8f-70cd39b42c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(df[\"zerrors\"]))\n",
    "print(max(df[\"pdierrors\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5cf5f-8c20-4ab1-92a0-c35b377bdd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code1(0.02,0.06,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f87bb-17a7-4626-ba4e-130444c7c5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2703992-1132-4347-90d1-161ea33fce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 5.1 - Adding random multiplicative errors to the dataset and modelling the unimodal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1106f-c21f-4f47-94a5-80e2d6c57803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "\n",
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50ec87-218f-4502-8cfe-abecd62f4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(64, input_shape=(1,190))) #return_sequences=True\n",
    "    #model.add(LSTM(16))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59de04-1b3a-4d56-9204-800e43a041c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 3.094820931437425e-05\n",
    "\n",
    "df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) (errors) = 0.007195653749104924\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) (errors) = 0.020737031791626045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005524d9-0b46-4716-a748-4ff760f23372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_mulerrors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_mulerrors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5fd91-6b81-409a-8c87-f826631f919b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ada9b-bb7f-467a-8193-0d6edb409c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 5.2 - Adding random additive errors to the dataset and modelling the unimodal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f314f-b10e-4727-b580-6f639cbc4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "\n",
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283be5d-c042-4869-99a2-ae80c221b013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(64, input_shape=(1,190))) #return_sequences=True\n",
    "    #model.add(LSTM(16))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae4fa0-9e99-439d-92b2-c87b920628f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 3.094820931437425e-05\n",
    "\n",
    "df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) (errors) = 0.007195653749104924\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) (errors) = 0.020737031791626045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0316ea8d-df45-486b-8b22-f9e888cea5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_adderrors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_adderrors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1a962-5ce1-415a-8989-a65cf7353cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31b470-3161-44b4-9429-e29082a46d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60ad70-5f87-4896-8ebe-b24017879939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a697b59-cf61-4699-8b8d-ba33d4d2684d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd56e58-3b35-4cca-883e-dba1e143b17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f8d59-8630-44e9-8170-6c97c83f0743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43214fa7-9497-4a58-b9b1-285043cf8909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9e37f-29c7-4764-a808-29e1f38f8b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef712b92-8cab-433b-93c3-27c1e7a63234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 5.3 - Adding random errors and random multiplicative errors to the dataset and modelling the unimodal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3c238-4ac1-40f1-8aa0-c8162ad78e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39643f-0876-4688-a445-cf18d0f87146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453da2d0-e79c-4875-84ae-105efad5b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9c395-7da5-416b-aa49-444915c20d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8133a7-78ab-468f-8983-a9c194bfa122",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f4a5f-5eb7-4f97-b57d-f8c6fba8a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79bf14-4fe8-451e-a915-fb14e9469cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1603636-723e-4d98-9ed1-6558b6ba3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc72a9-2afb-488c-81cb-4bd5f1cb49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.05, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca17c4-db3e-4e4a-a290-be43029c10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a265d9-0b57-437c-aab2-3a86c61d6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X + noise #additive errors (element wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634257b0-6aa9-417b-a02d-ee8480227ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667bff8-977a-4b9f-ad13-4aa2c558c1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(64, input_shape=(1,190))) #return_sequences=True\n",
    "    #model.add(LSTM(16))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905f090-8a13-4c5c-8412-22e0546568f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 3.094820931437425e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89b8ab-b751-415c-8277-1147c0199a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a4595-38ba-4dd9-afb9-ac98332bfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd1b8f-af76-435e-9624-9399330b35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_randomerrors.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb63fef-0821-4ffa-a507-961c037584b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) (errors) = 0.007195653749104924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b24e84-80b9-465c-9986-c627e788fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) (errors) = 0.020737031791626045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc5ce5-360b-4a43-a32c-2d5cfb4a7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b7927-0b0b-4c0c-98fd-b36fe0a208d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d539a-e3d9-49e8-9362-66d6fda3e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 6.1 - Error analysis of unimodal with errors - Run Code 2 and change the model accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da65c1d-1884-4f34-b84a-d82b3af71622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7834b46-a521-467b-8d89-f240c934e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a07de9-2961-4dca-b944-ccec085d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f12608-649b-44ed-866e-00e9a7ebed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc58e4c-7410-43e9-a19c-18a02d1fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = len(df[df['pdierrors']>0.06]['pdierrors'])/len(df['pdierrors'])\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a6629-6381-404f-a086-e8c73d4993eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "code1(0.02,0.06,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a0e94-b2d0-4735-af77-8ca465530020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 6.2 - Error analysis of unimodal with errors (Mul, Add and Both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049748af-0d58-451e-bd5a-6c542457982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08277c-df2c-4152-9256-db52c4f2589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = ['C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_mulerrors.csv',\n",
    "            'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_adderrors.csv',\n",
    "            'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv']\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c35a7f-23e5-4ca6-89f1-5c355dc3aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with multiplicative and additive errors')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with additive errors')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='purple',label='Prediction with multiplicative errors')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969887f-5493-44d0-9e5b-c249fdde09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with multiplicative and additive errors')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with additive errors')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='purple',label='Prediction with multiplicative errors')\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140d5cc-0315-4f99-9e45-45dfc516d022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\"]\n",
    "loadings = ['C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_mulerrors.csv',\n",
    "            'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_adderrors.csv',\n",
    "            'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv']\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a3b12-adf6-417f-afa1-3e8c8927fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with multiplicative and additive errors')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with additive errors')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='purple',label='Prediction with multiplicative errors')\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10273e-c1e0-4658-b45c-80d6b748ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with multiplicative and additive errors')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with additive errors')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='purple',label='Prediction with multiplicative errors')\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d29fe-c3b2-41d9-9f55-db663441f3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f7059-1976-4a2a-b027-36cddc1cb2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f2aad-cfb7-4374-b720-528a8a10e04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b8310-43da-4573-bdbf-b8bec5247e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the error graphs together (Original, Restricted, Error induced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f76b67-2db3-4c66-b5c4-ea960c7b0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721d433-0476-4988-8abe-1a65b4504fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f21b5-226c-4aaa-b4e4-35d7bf0afc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839d442-909e-4247-a015-7b3d9d2247d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d8805-1782-4f3f-9f06-eddf6a1a2363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798350a-027e-455d-83fc-8ad71b9affce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209fc5b-9c92-4c5d-9cc6-8eba9d4ef0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da12d8-aef2-44cc-aff5-6e1a9423c3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce610a7-b5e0-4236-9b29-91c39ced2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 7 - Analyzing errors in modelling Z and PDI with restricted ranges (low, mid, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33a88d-e540-413d-90e0-78c84e749b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f3ba6-ae64-451d-9e0d-b7b67e5702f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]  # 190 explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da228d87-440e-4c1b-989e-87cc6813f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross1 = []\n",
    "flag = 0 \n",
    "for i, row in X.iterrows():\n",
    "    for j in range(len(row)-95):\n",
    "        #print(row)\n",
    "        if row.iloc[j] > row.iloc[j+95] and flag==0:\n",
    "            flag = 1\n",
    "            cross1.append(j)\n",
    "            break\n",
    "    if flag == 0:\n",
    "        cross1.append(-1)\n",
    "    flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b6932-59ef-4b0c-b2a6-005837047d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cross1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a09bd-7aac-494f-bb80-3e28269418c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross2 = []\n",
    "flag = 0 \n",
    "for i, row in X.iterrows():\n",
    "    for j in range(len(row)-95-1,-1,-1):\n",
    "        #print(row)\n",
    "        if row.iloc[j] > row.iloc[j+95] and flag==0:\n",
    "            flag = 1\n",
    "            cross2.append(j)\n",
    "            break\n",
    "    if flag == 0:\n",
    "        cross2.append(-1)\n",
    "    flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf0a85-0917-42e7-8a0a-54e678c3d9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cross2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661b02d-e8d0-4dc8-9bfd-0008016360a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {}\n",
    "\n",
    "for item in cross1:\n",
    "    if item in frequency_dict:\n",
    "        frequency_dict[item] += 1\n",
    "    else:\n",
    "        frequency_dict[item] = 1\n",
    "\n",
    "print(frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7519df7-f3a0-42db-ac62-3c9ad883fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {}\n",
    "\n",
    "for item in cross2:\n",
    "    if item in frequency_dict:\n",
    "        frequency_dict[item] += 1\n",
    "    else:\n",
    "        frequency_dict[item] = 1\n",
    "\n",
    "print(frequency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459914a-76c2-4e71-b10f-e8cc87c23782",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross1copy = cross1.copy()\n",
    "cross2copy = cross2.copy()\n",
    "cross1copy = [item for item in cross1copy if item != -1]\n",
    "cross2copy = [item for item in cross2copy if item != -1]\n",
    "print(sum(cross1copy)/len(cross1copy))\n",
    "print(sum(cross2copy)/len(cross2copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c78c3-9215-461b-a211-bf9bb0deeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So 1st split is : 0 to 46\n",
    "# 2nd split is : 47 to 83\n",
    "# 3rd split is 84 to 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f569c5-38d5-41a0-8be6-d07e3dffddf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c1dbb-b48a-4ca5-a06d-6cd0532138fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d087823-803a-4075-ab6b-ce641d6ecea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "X1 = X.iloc[:, 0:46+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95:95+47]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae009f-16eb-4eae-980a-09ee2f522aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddcd5c-0653-4b00-9e41-31c4d2a1f60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,94),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223505e7-cd39-438b-a01c-4b7b7de973b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e0f40-b26c-40fc-8cd3-ebbe0d3637a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d204780-2af4-4968-8342-43bedf6d747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_low.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176bdc0-51f2-4e98-ae24-ad855675df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa7436-e299-4ee2-9ce2-2e2056328368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928fcc0-d392-44a0-a0f4-ac2fd02be7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1c210-eac6-40f9-b74b-c0483b2e7015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719ddfb-4c1c-449a-8d06-9a2386bcd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Middle Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bb837-4c52-42c3-86ab-a55488631ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "X1 = X.iloc[:, 47:83+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95 + 47:95+83+1]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164c3ae-0414-47d8-a34f-5c1a10630ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,74),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfac37-f21d-40f3-aa15-7e30a12201f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529dc54d-4aa3-47cf-8146-f08b9bc50e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c940ebd-0297-4ab5-b1e0-cb807e5fb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7f127-0bb0-4e04-96fd-ef6d3882321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_mid.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1754a-5857-4c16-ae11-a3f9a96f26ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d145b5a-b9b1-409a-9700-82351e5417f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec388fd5-2d19-40e2-81ad-30b9a8bf5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "X1 = X.iloc[:, 84:94+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95 + 84:95+94+1]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985fcd6-00fe-43ad-ab91-1eca3927a9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,22),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d6fd3-9075-4ead-847d-e33fc563f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be55a4-7301-4901-b3a7-fe99045d775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "z_mae\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044f1a4-ac6b-4e5d-b421-6cae17737268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "pdi_mae\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b51cc1-efe9-4287-892f-eeb5087b3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_high.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799a1ae-65c8-4a54-a006-6e7b2faddb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dbe79-c1bb-433b-a04a-19e49957984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 8 - Analyzing the errors of restricted modelling Low, Med and High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf7b72-a26d-440a-ba57-a89ff5c0c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d6604-6446-406b-b683-fbe46acdfcec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761525c1-e756-4501-bdd2-df2b4d6daa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range')\n",
    "plt.plot(POINTS[7], NEWDATA[7],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa10d51-8aa0-428f-821b-4486f963d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range')\n",
    "plt.plot(POINTS[8], NEWDATA[8],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a34d03-546b-4f3d-a2fe-d90108e34d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30639cf6-e926-4d2f-8dbd-bf9d759660b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt Z\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range')\n",
    "plt.plot(POINTS[7], NEWDATA[7],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea7079-e110-443c-a11a-0c6d0571bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt PDI\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range')\n",
    "plt.plot(POINTS[8], NEWDATA[8],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437599db-cf0b-41b3-895d-7d5483a563cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc90ee1-bdf0-48b4-bef2-f2fdd7d3894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 9 - Adding errors to the restricted frequency bands and seeing how model reacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c178e-7045-414c-9bb2-54cf567312b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbedc00-7283-4e34-839d-95335e0f3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "\n",
    "\n",
    "#X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "X1 = X.iloc[:, 0:46+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95:95+47]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a83226-733c-4d1d-b8f6-a646920a3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,94),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd4f64-4706-4b5a-9fe6-f46f4913927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "print(pdi_mae)\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbec75-3909-48bb-9516-9b50193a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_low_errors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c08bd-499b-42c3-87a3-c5f05c0856f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e9a42-fc00-4ddd-8860-cf05f1afd9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81efe8-0be8-4d46-be3d-37d71fc92212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "\n",
    "X1 = X.iloc[:, 47:83+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95 + 47:95+83+1]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3307f4e-97cd-471c-b7e0-20f10fbe1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,74),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd3227-0113-495a-a064-4361235965a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "print(pdi_mae)\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbd5d7-369f-460c-86e2-4cf7d75a35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_mid_errors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e235a-c494-4599-99f3-028b6d3712cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa1812-eaec-4936-a8ac-88b87ba78aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb6069-05d8-4a7f-a041-0d2934053fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "\n",
    "X1 = X.iloc[:, 84:94+1]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = X.iloc[:, 95 + 84:95+94+1]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836db091-b68c-47e9-a2fb-bc1e63c4c3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,22),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61301563-2e28-42e4-a1e4-77d640eeb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.00015784671995788813\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (MLP) =\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "#Best Z_mae (RNN) = 0.003330880872368445 (restricted)\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "print(pdi_mae)\n",
    "\n",
    "#Best PDI_mae (MLP) = \n",
    "#Best PDI_mae (RNN) = 0.003953151289141093\n",
    "#Best PDI_mae (RNN) = 0.005428341752105807 (restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaeeefd-6138-43b8-95d5-faa180333efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_restricted_high_errors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ceb6df-89e8-4518-9611-3c85ae0bf996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a01ef-8ef7-4d2c-ac46-f0ef61869676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 10 - Analyzing the difference in errors in prediction of limited freq trained data (low, mid, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dd165-a0c2-43dc-b1c8-cf892231c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1c177-98c4-4a29-bfcf-b468118c6dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\n",
    "            \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",\n",
    "           \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low_errors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid_errors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high_errors.csv\",\n",
    "           \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fcb20-8854-4701-8317-1269bd05b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt Z\n",
    "plt.plot(POINTS[13], NEWDATA[13],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range with errors')\n",
    "plt.plot(POINTS[11], NEWDATA[11],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range with errors')\n",
    "plt.plot(POINTS[9], NEWDATA[9],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range with errors')\n",
    "plt.plot(POINTS[15], NEWDATA[15],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range with errors')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fc591-4a84-4622-afed-a70beeced38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt PDI\n",
    "plt.plot(POINTS[14], NEWDATA[14],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range with errors')\n",
    "plt.plot(POINTS[12], NEWDATA[12],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range with errors')\n",
    "plt.plot(POINTS[10], NEWDATA[10],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range with errors')\n",
    "plt.plot(POINTS[16], NEWDATA[16],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range with errors')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948138a-481e-4c53-a4d4-191efe951ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\"]\n",
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high.csv\",\n",
    "            \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",\n",
    "           \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_low_errors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_mid_errors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted_high_errors.csv\",\n",
    "           \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63201648-8902-4e34-931a-330e71624367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt Z\n",
    "plt.plot(POINTS[13], NEWDATA[13],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range with errors')\n",
    "plt.plot(POINTS[11], NEWDATA[11],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range with errors')\n",
    "plt.plot(POINTS[9], NEWDATA[9],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range with errors')\n",
    "plt.plot(POINTS[15], NEWDATA[15],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range with errors')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef8881-a94f-4dec-aeae-db2db3afd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt PDI\n",
    "plt.plot(POINTS[14], NEWDATA[14],marker='o', linestyle='-', color='red',label='Prediction with restricted high frequency range with errors')\n",
    "plt.plot(POINTS[12], NEWDATA[12],marker='o', linestyle='-', color='b',label='Prediction with restricted mid frequency range with errors')\n",
    "plt.plot(POINTS[10], NEWDATA[10],marker='o', linestyle='-', color='purple',label='Prediction with restricted low frequency range with errors')\n",
    "plt.plot(POINTS[16], NEWDATA[16],marker='o', linestyle='-', color='lime',label='Prediction with full frequency range with errors')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1b575-1cf8-4d4c-a8eb-42b59cdcc292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c11d71-5d83-4d6b-b091-2c1b9aa03785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 11 - Unclean data predictions on cleanly trained model and vice versa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f0b60-9ca8-41ec-80ce-ddd990e4300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "def trying(moodel, deteset, texts):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "    data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "    \n",
    "    X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "    y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\":\n",
    "        noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "        # noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "        # noise3 = noise + noise2\n",
    "        X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "        noise = np.random.normal(0, 0.05, X.shape)\n",
    "        X = X + noise #additive errors (element wise)\n",
    "        \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    xcopytest = X_test\n",
    "    \n",
    "    # Standardize the data (mean=0, variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #X_trainNP = X_train.flatten()\n",
    "    y_trainNP = y_train.to_numpy()\n",
    "    \n",
    "    # Load the model\n",
    "    loaded_model = tf.keras.models.load_model(moodel)\n",
    "    \n",
    "    X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    # Make predictions with the loaded model\n",
    "    y_pred_loaded = loaded_model.predict(X_test_reshaped)\n",
    "    \n",
    "    df1 = pd.DataFrame(xcopytest,columns = X.columns)\n",
    "    df1 = df1.reset_index(drop=False)\n",
    "    df1[\"index\"] = df1[\"index\"] + 2\n",
    "    df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "    df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "    df = df1.join(df2).join(df3)   \n",
    "    \n",
    "    df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "    z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "    print(z_mae)\n",
    "    \n",
    "    #Best Z_mae (MLP) = 0.0035236082972914915\n",
    "    #Best Z_mae (RNN) = 0.0017323339878604644\n",
    "    \n",
    "    df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "    pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "    print(pdi_mae)\n",
    "    \n",
    "    #Best PDI_mae (MLP) = \n",
    "    #Best PDI_mae (RNN) = 0.003953151289141093\n",
    "    #Best PDI_mae (RNN) = 0.005428341752105807 (restricted)\n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\":\n",
    "        deteset2 = \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\"\n",
    "    elif deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\":\n",
    "        deteset2 = \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\"\n",
    "\n",
    "    loadings = [0,deteset2]\n",
    "    count = 0\n",
    "    NEWDATA = [[]]\n",
    "    POINTS = [[]]\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"],[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    for l in loadings:\n",
    "        if l!=0:\n",
    "            df = pd.read_csv(l)\n",
    "        for s in strings:\n",
    "        \n",
    "            \n",
    "            val = min(df[s[0]])\n",
    "            points = [val]\n",
    "            while val + 0.0625*2 < max(df[s[0]]):\n",
    "                val = val + 0.0625*2\n",
    "                points.append(val)\n",
    "            points.append(max(df[s[0]]))\n",
    "        \n",
    "            #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "            newdata = []\n",
    "            for i,c in enumerate(points[:-1]):\n",
    "                if i!=len(points)-1:\n",
    "                    arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "                else:\n",
    "                    arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "            newdata\n",
    "            #newdataZ = newdata\n",
    "            points = [x + 0.0625 for x in points[:-1]]\n",
    "            POINTS.append(points)\n",
    "            NEWDATA.append(newdata)\n",
    "            #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "            #plt.show()\n",
    "\n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\":\n",
    "        extra = 0\n",
    "    else:\n",
    "        extra = 2\n",
    "\n",
    "    #Z errors wrt Z\n",
    "    plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='red',label=texts[0+extra])\n",
    "    plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='lime',label=texts[1+extra])\n",
    "    # naming the x axis\n",
    "    plt.xlabel(r'$\\overline{Z}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "    # giving a title to my graph\n",
    "    plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #PDI errors wrt PDI\n",
    "    plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='red',label=texts[0+extra])\n",
    "    plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='lime',label=texts[1+extra])\n",
    "    # naming the x axis\n",
    "    plt.xlabel(r'$\\overline{PDI}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "    # giving a title to my graph\n",
    "    plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #PDI errors wrt Z\n",
    "    plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='red',label=texts[0+extra])\n",
    "    plt.plot(POINTS[7], NEWDATA[7],marker='o', linestyle='-', color='lime',label=texts[1+extra])\n",
    "    # naming the x axis\n",
    "    plt.xlabel(r'$\\overline{Z}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "    # giving a title to my graph\n",
    "    plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Z errors wrt PDI\n",
    "    plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='red',label=texts[0+extra])\n",
    "    plt.plot(POINTS[8], NEWDATA[8],marker='o', linestyle='-', color='lime',label=texts[1+extra])\n",
    "    # naming the x axis\n",
    "    plt.xlabel(r'$\\overline{PDI}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "    # giving a title to my graph\n",
    "    plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116262b-a422-446f-b280-c7a9cc410271",
   "metadata": {},
   "outputs": [],
   "source": [
    "textss = [\"Prediction of clean model on unclean data\",\"Prediction of clean model on clean data\",\"Prediction of unclean model on clean data\",\"Prediction of unclean model on unclean data\"]\n",
    "#textss = [\"Prediction of unclean data on clean model\",\"Prediction of clean data on clean model\",\"Prediction of clean data on unclean model\",\"Prediction of unclean data on unclean model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dc5bd-7244-42ee-bc83-7b03934ba8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trying(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model.h5\", \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\",textss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51087aa-4536-4689-894a-4aa5f6f1de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trying(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_randomerrors.h5\", \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",textss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec89c7-46a9-4986-be3f-8a08133a3286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f5399-4495-4e2f-9e12-13e2998987ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 12 - Working on \"Real life data\", ie restricted freq range and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988d17-544d-4f95-9bdf-0a8170c7c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\uni_PE_wint_2_low_uniform_200000_training.csv\")  # Replace with your CSV file path\n",
    "data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "\n",
    "#X = data.iloc[:, 2:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :2]  # 2 numerical variables to predict\n",
    "\n",
    "X1 = data.iloc[:, 22:49]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = data.iloc[:, 117:144]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (202500, 1))\n",
    "# noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "# noise3 = noise + noise2\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "        \n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe530e6-d3b2-4b28-8398-274dbbafc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(100, input_shape=(1,54),return_sequences=True)) #return_sequences=True\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.25))\n",
    "    # model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c117ee-71e9-4586-9cbf-bd5a3d182d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model(model)\n",
    "\n",
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "df1 = pd.DataFrame(xcopytest,columns = X.columns)\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"zerrors\"] = (df[\"Z\"] - df[\"ZPRED\"]).abs()\n",
    "z_mae =sum(df[\"zerrors\"])/len(df[\"zerrors\"])\n",
    "print(z_mae)\n",
    "\n",
    "#Best Z_mae (MLP) = 0.0035236082972914915\n",
    "#Best Z_mae (RNN) = 0.0017323339878604644\n",
    "\n",
    "df[\"pdierrors\"] = (df[\" PDI\"] - df[\" PDIPRED\"]).abs()\n",
    "pdi_mae =sum(df[\"pdierrors\"])/len(df[\"pdierrors\"])\n",
    "print(pdi_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3a746-8fb7-4250-97e2-8ccd9dd0c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_realdata.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_realdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253996b-0faa-40d4-aa43-f36892e1f235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2851fa-6b7f-447f-85ea-45e491e0c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 13 - Error analysis on pseudo real life data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b29b36-0a2f-4ac9-a151-efb15db09943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_realdata.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b6661-4d72-4510-8437-a415b2755e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7dafe-2457-4882-9459-e22c3f37b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['zerrors']>0.06]['zerrors'])/len(df['zerrors']))\n",
    "print(len(df[df['pdierrors']>0.06]['pdierrors'])/len(df['pdierrors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6914c-2a3c-4912-b993-bb84b1cf5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "code1(0.08,0.12,0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa39245-40e9-407c-a0b9-b52af967203f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e64617-4828-400b-a615-48ddde81e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the error graphs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404924c7-7a60-4109-83c3-d2c8749300fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_restricted.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_randomerrors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_realdata.csv\"]\n",
    "count = 0\n",
    "NEWDATA = [[]]\n",
    "POINTS = [[]]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    strings = [[\"Z\",\"z\"], [\" PDI\",\"pdi\"],[\"Z\",\"pdi\"], [\" PDI\",\"z\"]]\n",
    "    \n",
    "    for s in strings:\n",
    "    \n",
    "        \n",
    "        val = min(df[s[0]])\n",
    "        points = [val]\n",
    "        while val + 0.0625*2 < max(df[s[0]]):\n",
    "            val = val + 0.0625*2\n",
    "            points.append(val)\n",
    "        points.append(max(df[s[0]]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s[0]]>=points[i]][df[s[0]]<=points[i+1]][s[1]+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        #newdataZ = newdata\n",
    "        points = [x + 0.0625 for x in points[:-1]]\n",
    "        POINTS.append(points)\n",
    "        NEWDATA.append(newdata)\n",
    "        #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d216666-51a2-4809-b4e0-123d97db8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z\n",
    "plt.plot(POINTS[13], NEWDATA[13],marker='o', linestyle='-', color='purple',label='Prediction with restricted frequency and artificial errors induced')\n",
    "plt.plot(POINTS[9], NEWDATA[9],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[5], NEWDATA[5],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[1], NEWDATA[1],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12199fcc-329c-4e60-9745-588bb979ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI\n",
    "plt.plot(POINTS[14], NEWDATA[14],marker='o', linestyle='-', color='purple',label='Prediction with restricted frequency and artificial errors induced')\n",
    "plt.plot(POINTS[10], NEWDATA[10],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[6], NEWDATA[6],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[2], NEWDATA[2],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13548957-5d0d-4b6f-a96d-b6665d28574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDI errors wrt Z\n",
    "plt.plot(POINTS[15], NEWDATA[15],marker='o', linestyle='-', color='purple',label='Prediction with restricted frequency and artificial errors induced')\n",
    "plt.plot(POINTS[11], NEWDATA[11],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[7], NEWDATA[7],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[3], NEWDATA[3],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{Z}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{PDI} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{PDI} \\text{ Errors vs } \\overline{Z}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d180e-88c9-4334-86f3-ffe6ce68ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z errors wrt PDI\n",
    "plt.plot(POINTS[16], NEWDATA[16],marker='o', linestyle='-', color='purple',label='Prediction with restricted frequency and artificial errors induced')\n",
    "plt.plot(POINTS[12], NEWDATA[12],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "plt.plot(POINTS[8], NEWDATA[8],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "plt.plot(POINTS[4], NEWDATA[4],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "# naming the x axis\n",
    "plt.xlabel(r'$\\overline{PDI}$')\n",
    "# naming the y axis\n",
    "plt.ylabel(r'$\\overline{Z} \\text{ Errors}$') \n",
    "# giving a title to my graph\n",
    "plt.title(r'$\\overline{Z} \\text{ Errors vs } \\overline{PDI}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850709b-3c6b-4304-a344-808d95c5874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26c0be-6845-4872-ae80-a59563180702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21ce24-4f8e-4ef1-98b5-6e0353216126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 14 - Modelling for Bi - modal case (Pred Z1, Z2, PDI1, PDI2 and Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab473408-329b-4b93-98ef-5b2d621a3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfc84a-29bb-4b08-b31e-944bd2842196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4760d13-6d56-48f6-8d97-37cad04d7d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a59ce-ed3f-41e6-a799-33c47bfc84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efd2d1-5a7f-42c8-9ad4-4cb84c06ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e2684-9de5-419e-89ce-65df92b72ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 5:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf7fd9-fa31-48d2-8d2b-642ee4318ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba5b9e-c672-4580-bd41-4a67c96572d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis = []\n",
    "# for i in ['Zs']:\n",
    "#   lis.append(y[i].unique())\n",
    "\n",
    "# lis = [item for sublist in lis for item in sublist]\n",
    "# for P in lis:\n",
    "#     c=0\n",
    "#     for i in y['Zs']:\n",
    "#       if i==P:\n",
    "#         c = c+1\n",
    "#     print(P,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff336f3-0261-41e7-940b-106c55cb77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[\"sl\"] = y[\"Zs\"] + y[\"Zl\"]\n",
    "# plt.hist(y[\"sl\"])\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866e6a0-1396-4201-8355-2f0e50364c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97a824-816d-4e0a-a54e-1968f05956e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.iloc[:,:5]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac68e3-885e-40e1-84d2-0a88b4e94e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.read_csv('run0.dat', delim_whitespace=True,header = 0)\n",
    "freq = np.log10(freq['#w'])\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0732b-8ddb-4469-9e0f-6e8156e17163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zs vs PDI_s\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "def nearest(points,reference_points):\n",
    "    # Define the set of 5D points (each row is a point)\n",
    "    # points = np.array([\n",
    "    #     [1, 2, 3, 4, 5],\n",
    "    #     [2, 3, 4, 5, 6],\n",
    "    #     [0, 0, 0, 0, 0],\n",
    "    #     [1, 1, 1, 1, 1],\n",
    "    #     [5, 5, 5, 5, 5]\n",
    "    # ])\n",
    "    points = np.asarray(points)\n",
    "    # Define a reference point (e.g., the mean of all points or another specific point)\n",
    "    # Here, using the mean point as the reference\n",
    "    # reference_point = np.mean(points, axis=0)\n",
    "    for i,reference_point in enumerate(reference_points):\n",
    "        \n",
    "        # Calculate the absolute error for each point\n",
    "        absolute_errors = np.sum(np.abs(points - reference_point), axis=1)\n",
    "        #print(absolute_errors)\n",
    "        # Find the index of the point with the least absolute error\n",
    "        min_error_index = np.argmin(absolute_errors)\n",
    "        #print(min_error_index)\n",
    "        # Get the 5D point with the least absolute error\n",
    "        #print(points)\n",
    "        point_with_least_error = points[min_error_index,:]\n",
    "        # print(i)\n",
    "        # print(point_with_least_error)\n",
    "        Zpercentiles[i] = point_with_least_error[0]\n",
    "        otherZpercentiles[i] = point_with_least_error[1]\n",
    "        PDIpercentiles[i] = point_with_least_error[2]\n",
    "        otherPDIpercentiles[i] = point_with_least_error[3]\n",
    "        phipercentiles[i] = point_with_least_error[4]\n",
    "    \n",
    "    # print(\"Reference Point:\", reference_point)\n",
    "    # print(\"Point with the least absolute error:\", point_with_least_error)\n",
    "    # print(\"Minimum absolute error:\", absolute_errors[min_error_index])\n",
    "\n",
    "# reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "# nearest(data[,:5],reference_points)\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 4, 5.5, 8, 10])\n",
    "# targets = [5,11,12]\n",
    "# nearest_value = find_nearest(data, targets)\n",
    "# print(f\"The nearest value to {target} is {nearest_value}.\")\n",
    "y = \"Zs\"\n",
    "x = \"PDI_s\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "Zpercentiles = np.percentile(data[y], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "Zpercentiles = [element for element in Zpercentiles for _ in range(5)]\n",
    "# Zpercentiles = Zpercentilesr\n",
    "PDIpercentiles = np.percentile(data[x], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "PDIpercentiles = np.tile(PDIpercentiles, 5)\n",
    "# print(PDIpercentilesr)\n",
    "# PDIpercentiles = PDIpercentilesr\n",
    "#other percentiles \n",
    "phipercentiles = np.percentile(data[\"phiL\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "otherZpercentiles = np.percentile(data[\"Zl\"], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "otherPDIpercentiles = np.percentile(data[\"PDI_l\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "# print(otherZpercentiles[2])\n",
    "otherZpercentiles = [otherZpercentiles[2]] * 25\n",
    "# print(otherZpercentiles)\n",
    "otherPDIpercentiles = [otherPDIpercentiles[2]] * 25\n",
    "phipercentiles = [phipercentiles[2]] * 25\n",
    "# print(Zpercentiles)\n",
    "# print(data[\"Z\"])\n",
    "# Zpercentiles = find_nearest(data[y], Zpercentiles)\n",
    "# PDIpercentiles = find_nearest(data[x], PDIpercentiles)\n",
    "# otherZpercentiles = find_nearest(data[\"Zl\"], otherZpercentiles)\n",
    "# otherPDIpercentiles = find_nearest(data[\"PDI_l\"], otherPDIpercentiles)\n",
    "# phipercentiles = find_nearest(data[\"phiL\"], phipercentiles)\n",
    "# print(len(Zpercentiles), len(otherZpercentiles),len( PDIpercentiles), len(otherPDIpercentiles), len(phipercentiles))\n",
    "reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "d = data.iloc[:,:5]\n",
    "nearest(d,reference_points)\n",
    "# print(Zpercentiles[5], otherZpercentiles[5], PDIpercentiles[5], otherPDIpercentiles[5], phipercentiles[5])\n",
    "# print(Zpercentiles)\n",
    "terms = [\"Very Low\",\"Low\",\"Mid\",\"High\",\"Very High\"]\n",
    "for term1,i in enumerate(Zpercentiles):\n",
    "    # needrowGprime = pd.DataFrame()\n",
    "    # needrowGdprime = pd.DataFrame()\n",
    "    needrowGprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 5:100]\n",
    "    needrowGdprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 100:]\n",
    "    # print(needrowGprime)\n",
    "    # print(needrowGdprime)\n",
    "    # print(data[data[\"Z\"]==i].iloc[:5, :7])\n",
    "    # print(data[data[\" PDI\"]==j].iloc[:5, :7])\n",
    "    #Z errors wrt PDI\n",
    "    print(len(freq),len(needrowGprime.iloc[0].values),len(needrowGdprime.iloc[0].values))\n",
    "    plt.plot(freq, needrowGprime.iloc[0].values,marker='o', linestyle='-', color='red',label=f\"G' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.plot(freq, needrowGdprime.iloc[0].values,marker='o', linestyle='-', color='b',label=f\"G'' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.xlabel(r'$\\overline{\\omega}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r\"$\\overline{G'} \\text{ and } \\overline{G''}$\")\n",
    "     \n",
    "    # giving a title to my graph\n",
    "    plt.title(f\"Flow Curve at {terms[int(term1/5)]} $\\\\overline{{{dict[y]}}}$ and {terms[term1%5]} $\\\\overline{{{dict[x]}}}$\")\n",
    "    plt.text(min(freq), max(max(needrowGprime.iloc[0].values), max(needrowGdprime.iloc[0].values))-1, f\"$\\\\overline{{{dict[y]}}} = {round(Zpercentiles[term1],2)}$  , $\\\\overline{{{dict[x]}}} = {round(PDIpercentiles[term1],2)}$  \", fontsize=10, color='black')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(int(term1/5),term1%5,y,x), format='png',dpi = 300)  # Save as PNG file #dpi = 600\n",
    "    plt.show()\n",
    "    # print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe4635-1bd2-42b9-be4b-d40a402b3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zs vs PDI_l\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "def nearest(points,reference_points):\n",
    "    # Define the set of 5D points (each row is a point)\n",
    "    # points = np.array([\n",
    "    #     [1, 2, 3, 4, 5],\n",
    "    #     [2, 3, 4, 5, 6],\n",
    "    #     [0, 0, 0, 0, 0],\n",
    "    #     [1, 1, 1, 1, 1],\n",
    "    #     [5, 5, 5, 5, 5]\n",
    "    # ])\n",
    "    points = np.asarray(points)\n",
    "    # Define a reference point (e.g., the mean of all points or another specific point)\n",
    "    # Here, using the mean point as the reference\n",
    "    # reference_point = np.mean(points, axis=0)\n",
    "    for i,reference_point in enumerate(reference_points):\n",
    "        \n",
    "        # Calculate the absolute error for each point\n",
    "        absolute_errors = np.sum(np.abs(points - reference_point), axis=1)\n",
    "        #print(absolute_errors)\n",
    "        # Find the index of the point with the least absolute error\n",
    "        min_error_index = np.argmin(absolute_errors)\n",
    "        #print(min_error_index)\n",
    "        # Get the 5D point with the least absolute error\n",
    "        #print(points)\n",
    "        point_with_least_error = points[min_error_index,:]\n",
    "        # print(i)\n",
    "        # print(point_with_least_error)\n",
    "        Zpercentiles[i] = point_with_least_error[0]\n",
    "        otherZpercentiles[i] = point_with_least_error[1]\n",
    "        PDIpercentiles[i] = point_with_least_error[2]\n",
    "        otherPDIpercentiles[i] = point_with_least_error[3]\n",
    "        phipercentiles[i] = point_with_least_error[4]\n",
    "    \n",
    "    # print(\"Reference Point:\", reference_point)\n",
    "    # print(\"Point with the least absolute error:\", point_with_least_error)\n",
    "    # print(\"Minimum absolute error:\", absolute_errors[min_error_index])\n",
    "\n",
    "# reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "# nearest(data[,:5],reference_points)\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 4, 5.5, 8, 10])\n",
    "# targets = [5,11,12]\n",
    "# nearest_value = find_nearest(data, targets)\n",
    "# print(f\"The nearest value to {target} is {nearest_value}.\")\n",
    "y = \"Zs\"\n",
    "x = \"PDI_l\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "Zpercentiles = np.percentile(data[y], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "Zpercentiles = [element for element in Zpercentiles for _ in range(5)]\n",
    "# Zpercentiles = Zpercentilesr\n",
    "otherPDIpercentiles = np.percentile(data[x], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "otherPDIpercentiles = np.tile(otherPDIpercentiles, 5)\n",
    "# print(PDIpercentilesr)\n",
    "# PDIpercentiles = PDIpercentilesr\n",
    "#other percentiles \n",
    "phipercentiles = np.percentile(data[\"phiL\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "otherZpercentiles = np.percentile(data[\"Zl\"], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "PDIpercentiles = np.percentile(data[\"PDI_s\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "# print(otherZpercentiles[2])\n",
    "otherZpercentiles = [otherZpercentiles[2]] * 25\n",
    "# print(otherZpercentiles)\n",
    "PDIpercentiles = [PDIpercentiles[2]] * 25\n",
    "phipercentiles = [phipercentiles[2]] * 25\n",
    "# print(Zpercentiles)\n",
    "# print(data[\"Z\"])\n",
    "# Zpercentiles = find_nearest(data[y], Zpercentiles)\n",
    "# PDIpercentiles = find_nearest(data[x], PDIpercentiles)\n",
    "# otherZpercentiles = find_nearest(data[\"Zl\"], otherZpercentiles)\n",
    "# otherPDIpercentiles = find_nearest(data[\"PDI_l\"], otherPDIpercentiles)\n",
    "# phipercentiles = find_nearest(data[\"phiL\"], phipercentiles)\n",
    "# print(len(Zpercentiles), len(otherZpercentiles),len( PDIpercentiles), len(otherPDIpercentiles), len(phipercentiles))\n",
    "reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "d = data.iloc[:,:5]\n",
    "nearest(d,reference_points)\n",
    "# print(Zpercentiles[5], otherZpercentiles[5], PDIpercentiles[5], otherPDIpercentiles[5], phipercentiles[5])\n",
    "# print(Zpercentiles)\n",
    "terms = [\"Very Low\",\"Low\",\"Mid\",\"High\",\"Very High\"]\n",
    "for term1,i in enumerate(Zpercentiles):\n",
    "    # needrowGprime = pd.DataFrame()\n",
    "    # needrowGdprime = pd.DataFrame()\n",
    "    needrowGprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 5:100]\n",
    "    needrowGdprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 100:]\n",
    "    # print(needrowGprime)\n",
    "    # print(needrowGdprime)\n",
    "    # print(data[data[\"Z\"]==i].iloc[:5, :7])\n",
    "    # print(data[data[\" PDI\"]==j].iloc[:5, :7])\n",
    "    #Z errors wrt PDI\n",
    "    print(len(freq),len(needrowGprime.iloc[0].values),len(needrowGdprime.iloc[0].values))\n",
    "    plt.plot(freq, needrowGprime.iloc[0].values,marker='o', linestyle='-', color='red',label=f\"G' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.plot(freq, needrowGdprime.iloc[0].values,marker='o', linestyle='-', color='b',label=f\"G'' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.xlabel(r'$\\overline{\\omega}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r\"$\\overline{G'} \\text{ and } \\overline{G''}$\")\n",
    "     \n",
    "    # giving a title to my graph\n",
    "    plt.title(f\"Flow Curve at {terms[int(term1/5)]} $\\\\overline{{{dict[y]}}}$ and {terms[term1%5]} $\\\\overline{{{dict[x]}}}$\")\n",
    "    plt.text(min(freq), max(max(needrowGprime.iloc[0].values), max(needrowGdprime.iloc[0].values))-1, f\"$\\\\overline{{{dict[y]}}} = {round(Zpercentiles[term1],2)}$  , $\\\\overline{{{dict[x]}}} = {round(PDIpercentiles[term1],2)}$  \", fontsize=10, color='black')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(int(term1/5),term1%5,y,x), format='png',dpi = 300)  # Save as PNG file #dpi = 600\n",
    "    plt.show()\n",
    "    # print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc1c78-2cfc-4307-a563-707458c973a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zl vs PDI_l\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "def nearest(points,reference_points):\n",
    "    # Define the set of 5D points (each row is a point)\n",
    "    # points = np.array([\n",
    "    #     [1, 2, 3, 4, 5],\n",
    "    #     [2, 3, 4, 5, 6],\n",
    "    #     [0, 0, 0, 0, 0],\n",
    "    #     [1, 1, 1, 1, 1],\n",
    "    #     [5, 5, 5, 5, 5]\n",
    "    # ])\n",
    "    points = np.asarray(points)\n",
    "    # Define a reference point (e.g., the mean of all points or another specific point)\n",
    "    # Here, using the mean point as the reference\n",
    "    # reference_point = np.mean(points, axis=0)\n",
    "    for i,reference_point in enumerate(reference_points):\n",
    "        \n",
    "        # Calculate the absolute error for each point\n",
    "        absolute_errors = np.sum(np.abs(points - reference_point), axis=1)\n",
    "        #print(absolute_errors)\n",
    "        # Find the index of the point with the least absolute error\n",
    "        min_error_index = np.argmin(absolute_errors)\n",
    "        #print(min_error_index)\n",
    "        # Get the 5D point with the least absolute error\n",
    "        #print(points)\n",
    "        point_with_least_error = points[min_error_index,:]\n",
    "        # print(i)\n",
    "        # print(point_with_least_error)\n",
    "        Zpercentiles[i] = point_with_least_error[0]\n",
    "        otherZpercentiles[i] = point_with_least_error[1]\n",
    "        PDIpercentiles[i] = point_with_least_error[2]\n",
    "        otherPDIpercentiles[i] = point_with_least_error[3]\n",
    "        phipercentiles[i] = point_with_least_error[4]\n",
    "    \n",
    "    # print(\"Reference Point:\", reference_point)\n",
    "    # print(\"Point with the least absolute error:\", point_with_least_error)\n",
    "    # print(\"Minimum absolute error:\", absolute_errors[min_error_index])\n",
    "\n",
    "# reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "# nearest(data[,:5],reference_points)\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 4, 5.5, 8, 10])\n",
    "# targets = [5,11,12]\n",
    "# nearest_value = find_nearest(data, targets)\n",
    "# print(f\"The nearest value to {target} is {nearest_value}.\")\n",
    "y = \"Zl\"\n",
    "x = \"PDI_l\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "otherZpercentiles = np.percentile(data[y], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "otherZpercentiles = [element for element in otherZpercentiles for _ in range(5)]\n",
    "# Zpercentiles = Zpercentilesr\n",
    "otherPDIpercentiles = np.percentile(data[x], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "otherPDIpercentiles = np.tile(otherPDIpercentiles, 5)\n",
    "# print(PDIpercentilesr)\n",
    "# PDIpercentiles = PDIpercentilesr\n",
    "#other percentiles \n",
    "phipercentiles = np.percentile(data[\"phiL\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "Zpercentiles = np.percentile(data[\"Zs\"], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "PDIpercentiles = np.percentile(data[\"PDI_s\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "# print(otherZpercentiles[2])\n",
    "Zpercentiles = [Zpercentiles[2]] * 25\n",
    "# print(otherZpercentiles)\n",
    "PDIpercentiles = [PDIpercentiles[2]] * 25\n",
    "phipercentiles = [phipercentiles[2]] * 25\n",
    "# print(Zpercentiles)\n",
    "# print(data[\"Z\"])\n",
    "# Zpercentiles = find_nearest(data[y], Zpercentiles)\n",
    "# PDIpercentiles = find_nearest(data[x], PDIpercentiles)\n",
    "# otherZpercentiles = find_nearest(data[\"Zl\"], otherZpercentiles)\n",
    "# otherPDIpercentiles = find_nearest(data[\"PDI_l\"], otherPDIpercentiles)\n",
    "# phipercentiles = find_nearest(data[\"phiL\"], phipercentiles)\n",
    "# print(len(Zpercentiles), len(otherZpercentiles),len( PDIpercentiles), len(otherPDIpercentiles), len(phipercentiles))\n",
    "reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "d = data.iloc[:,:5]\n",
    "nearest(d,reference_points)\n",
    "# print(Zpercentiles[5], otherZpercentiles[5], PDIpercentiles[5], otherPDIpercentiles[5], phipercentiles[5])\n",
    "# print(Zpercentiles)\n",
    "terms = [\"Very Low\",\"Low\",\"Mid\",\"High\",\"Very High\"]\n",
    "for term1,i in enumerate(Zpercentiles):\n",
    "    # needrowGprime = pd.DataFrame()\n",
    "    # needrowGdprime = pd.DataFrame()\n",
    "    needrowGprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 5:100]\n",
    "    needrowGdprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 100:]\n",
    "    # print(needrowGprime)\n",
    "    # print(needrowGdprime)\n",
    "    # print(data[data[\"Z\"]==i].iloc[:5, :7])\n",
    "    # print(data[data[\" PDI\"]==j].iloc[:5, :7])\n",
    "    #Z errors wrt PDI\n",
    "    print(len(freq),len(needrowGprime.iloc[0].values),len(needrowGdprime.iloc[0].values))\n",
    "    plt.plot(freq, needrowGprime.iloc[0].values,marker='o', linestyle='-', color='red',label=f\"G' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.plot(freq, needrowGdprime.iloc[0].values,marker='o', linestyle='-', color='b',label=f\"G'' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.xlabel(r'$\\overline{\\omega}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r\"$\\overline{G'} \\text{ and } \\overline{G''}$\")\n",
    "     \n",
    "    # giving a title to my graph\n",
    "    plt.title(f\"Flow Curve at {terms[int(term1/5)]} $\\\\overline{{{dict[y]}}}$ and {terms[term1%5]} $\\\\overline{{{dict[x]}}}$\")\n",
    "    plt.text(min(freq), max(max(needrowGprime.iloc[0].values), max(needrowGdprime.iloc[0].values))-1, f\"$\\\\overline{{{dict[y]}}} = {round(Zpercentiles[term1],2)}$  , $\\\\overline{{{dict[x]}}} = {round(PDIpercentiles[term1],2)}$  \", fontsize=10, color='black')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(int(term1/5),term1%5,y,x), format='png',dpi = 300)  # Save as PNG file #dpi = 600\n",
    "    plt.show()\n",
    "    # print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec988a5-1d56-4d82-b4e9-388fb5f7ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zl vs PDI_s\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "def nearest(points,reference_points):\n",
    "    # Define the set of 5D points (each row is a point)\n",
    "    # points = np.array([\n",
    "    #     [1, 2, 3, 4, 5],\n",
    "    #     [2, 3, 4, 5, 6],\n",
    "    #     [0, 0, 0, 0, 0],\n",
    "    #     [1, 1, 1, 1, 1],\n",
    "    #     [5, 5, 5, 5, 5]\n",
    "    # ])\n",
    "    points = np.asarray(points)\n",
    "    # Define a reference point (e.g., the mean of all points or another specific point)\n",
    "    # Here, using the mean point as the reference\n",
    "    # reference_point = np.mean(points, axis=0)\n",
    "    for i,reference_point in enumerate(reference_points):\n",
    "        \n",
    "        # Calculate the absolute error for each point\n",
    "        absolute_errors = np.sum(np.abs(points - reference_point), axis=1)\n",
    "        #print(absolute_errors)\n",
    "        # Find the index of the point with the least absolute error\n",
    "        min_error_index = np.argmin(absolute_errors)\n",
    "        #print(min_error_index)\n",
    "        # Get the 5D point with the least absolute error\n",
    "        #print(points)\n",
    "        point_with_least_error = points[min_error_index,:]\n",
    "        # print(i)\n",
    "        # print(point_with_least_error)\n",
    "        Zpercentiles[i] = point_with_least_error[0]\n",
    "        otherZpercentiles[i] = point_with_least_error[1]\n",
    "        PDIpercentiles[i] = point_with_least_error[2]\n",
    "        otherPDIpercentiles[i] = point_with_least_error[3]\n",
    "        phipercentiles[i] = point_with_least_error[4]\n",
    "    \n",
    "    # print(\"Reference Point:\", reference_point)\n",
    "    # print(\"Point with the least absolute error:\", point_with_least_error)\n",
    "    # print(\"Minimum absolute error:\", absolute_errors[min_error_index])\n",
    "\n",
    "# reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "# nearest(data[,:5],reference_points)\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 4, 5.5, 8, 10])\n",
    "# targets = [5,11,12]\n",
    "# nearest_value = find_nearest(data, targets)\n",
    "# print(f\"The nearest value to {target} is {nearest_value}.\")\n",
    "y = \"Zl\"\n",
    "x = \"PDI_s\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "otherZpercentiles = np.percentile(data[y], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "otherZpercentiles = [element for element in otherZpercentiles for _ in range(5)]\n",
    "# Zpercentiles = Zpercentilesr\n",
    "PDIpercentiles = np.percentile(data[x], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "PDIpercentiles = np.tile(PDIpercentiles, 5)\n",
    "# print(PDIpercentilesr)\n",
    "# PDIpercentiles = PDIpercentilesr\n",
    "#other percentiles \n",
    "phipercentiles = np.percentile(data[\"phiL\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "Zpercentiles = np.percentile(data[\"Zs\"], [0, 25, 50, 75, 100]) #ypercentiles\n",
    "otherPDIpercentiles = np.percentile(data[\"PDI_l\"], [0, 25, 50, 75, 100]) #xpercentiles\n",
    "# print(otherZpercentiles[2])\n",
    "Zpercentiles = [Zpercentiles[2]] * 25\n",
    "# print(otherZpercentiles)\n",
    "otherPDIpercentiles = [otherPDIpercentiles[2]] * 25\n",
    "phipercentiles = [phipercentiles[2]] * 25\n",
    "# print(Zpercentiles)\n",
    "# print(data[\"Z\"])\n",
    "# Zpercentiles = find_nearest(data[y], Zpercentiles)\n",
    "# PDIpercentiles = find_nearest(data[x], PDIpercentiles)\n",
    "# otherZpercentiles = find_nearest(data[\"Zl\"], otherZpercentiles)\n",
    "# otherPDIpercentiles = find_nearest(data[\"PDI_l\"], otherPDIpercentiles)\n",
    "# phipercentiles = find_nearest(data[\"phiL\"], phipercentiles)\n",
    "# print(len(Zpercentiles), len(otherZpercentiles),len( PDIpercentiles), len(otherPDIpercentiles), len(phipercentiles))\n",
    "reference_points = np.column_stack((Zpercentiles, otherZpercentiles, PDIpercentiles, otherPDIpercentiles, phipercentiles))\n",
    "d = data.iloc[:,:5]\n",
    "nearest(d,reference_points)\n",
    "# print(Zpercentiles[5], otherZpercentiles[5], PDIpercentiles[5], otherPDIpercentiles[5], phipercentiles[5])\n",
    "# print(Zpercentiles)\n",
    "terms = [\"Very Low\",\"Low\",\"Mid\",\"High\",\"Very High\"]\n",
    "for term1,i in enumerate(Zpercentiles):\n",
    "    # needrowGprime = pd.DataFrame()\n",
    "    # needrowGdprime = pd.DataFrame()\n",
    "    needrowGprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 5:100]\n",
    "    needrowGdprime = data[data[\"Zs\"]==Zpercentiles[term1]][data[\"PDI_s\"]==PDIpercentiles[term1]][data[\"Zl\"]==otherZpercentiles[term1]][data[\"PDI_l\"]==otherPDIpercentiles[term1]][data[\"phiL\"]==phipercentiles[term1]].iloc[:, 100:]\n",
    "    # print(needrowGprime)\n",
    "    # print(needrowGdprime)\n",
    "    # print(data[data[\"Z\"]==i].iloc[:5, :7])\n",
    "    # print(data[data[\" PDI\"]==j].iloc[:5, :7])\n",
    "    #Z errors wrt PDI\n",
    "    print(len(freq),len(needrowGprime.iloc[0].values),len(needrowGdprime.iloc[0].values))\n",
    "    plt.plot(freq, needrowGprime.iloc[0].values,marker='o', linestyle='-', color='red',label=f\"G' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.plot(freq, needrowGdprime.iloc[0].values,marker='o', linestyle='-', color='b',label=f\"G'' curve with {terms[int(term1/5)]} {y} and {terms[term1%5]} {x}\")\n",
    "    plt.xlabel(r'$\\overline{\\omega}$')\n",
    "    # naming the y axis\n",
    "    plt.ylabel(r\"$\\overline{G'} \\text{ and } \\overline{G''}$\")\n",
    "     \n",
    "    # giving a title to my graph\n",
    "    plt.title(f\"Flow Curve at {terms[int(term1/5)]} $\\\\overline{{{dict[y]}}}$ and {terms[term1%5]} $\\\\overline{{{dict[x]}}}$\")\n",
    "    plt.text(min(freq), max(max(needrowGprime.iloc[0].values), max(needrowGdprime.iloc[0].values))-1, f\"$\\\\overline{{{dict[y]}}} = {round(Zpercentiles[term1],2)}$  , $\\\\overline{{{dict[x]}}} = {round(PDIpercentiles[term1],2)}$  \", fontsize=10, color='black')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(int(term1/5),term1%5,y,x), format='png',dpi = 300)  # Save as PNG file #dpi = 600\n",
    "    plt.show()\n",
    "    # print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb8a09-b5b5-46bc-b910-4e6ad8594dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zs vs PDI_s\n",
    "y = \"Zs\"\n",
    "x = \"PDI_s\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Create a figure with subplots arranged in a grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "\n",
    "# Load and display each image in the grid\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(abs(4 - i),j,y,x)  # Path to the image file\n",
    "        img = mpimg.imread(img_path)  # Load the image\n",
    "        axs[i, j].imshow(img)  # Display the image\n",
    "        axs[i, j].axis('off')  # Hide the axis\n",
    "\n",
    "# Add overall x-axis and y-axis labels\n",
    "fig.supxlabel(f'$\\\\overline{{{dict[x]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.supylabel(f'$\\\\overline{{{dict[y]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.suptitle(f'Flow curves at different values of $\\\\overline{{{dict[y]}}}$ and $\\\\overline{{{dict[x]}}}$', fontsize=15)\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()  # Adjust bottom and top margins #rect=[0, 0, 1, 0.96]\n",
    "# Adjust layout to prevent overlap and increase spacing\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)  # Adjust space between subplots\n",
    "xspace = 0.06\n",
    "yspace = 0.04\n",
    "space = 0.0001\n",
    "plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "# Adding a note using fig.text\n",
    "fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\allgraph1.png', format='png',dpi=600)  # Save as PNG file\n",
    "plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f73333-ad11-4048-9b55-45f55a825b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zl vs PDI_s\n",
    "y = \"Zl\"\n",
    "x = \"PDI_s\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Create a figure with subplots arranged in a grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "\n",
    "# Load and display each image in the grid\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(abs(4 - i),j,y,x)  # Path to the image file\n",
    "        img = mpimg.imread(img_path)  # Load the image\n",
    "        axs[i, j].imshow(img)  # Display the image\n",
    "        axs[i, j].axis('off')  # Hide the axis\n",
    "\n",
    "# Add overall x-axis and y-axis labels\n",
    "fig.supxlabel(f'$\\\\overline{{{dict[x]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.supylabel(f'$\\\\overline{{{dict[y]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.suptitle(f'Flow curves at different values of $\\\\overline{{{dict[y]}}}$ and $\\\\overline{{{dict[x]}}}$', fontsize=15)\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()  # Adjust bottom and top margins #rect=[0, 0, 1, 0.96]\n",
    "# Adjust layout to prevent overlap and increase spacing\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)  # Adjust space between subplots\n",
    "xspace = 0.06\n",
    "yspace = 0.04\n",
    "space = 0.0001\n",
    "plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "# Adding a note using fig.text\n",
    "fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\allgraph2.png', format='png',dpi=300)  # Save as PNG file\n",
    "plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc1bb3-c292-409c-a472-c043bb27c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zs vs PDI_l\n",
    "y = \"Zs\"\n",
    "x = \"PDI_l\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Create a figure with subplots arranged in a grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "\n",
    "# Load and display each image in the grid\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(abs(4 - i),j,y,x)  # Path to the image file\n",
    "        img = mpimg.imread(img_path)  # Load the image\n",
    "        axs[i, j].imshow(img)  # Display the image\n",
    "        axs[i, j].axis('off')  # Hide the axis\n",
    "\n",
    "# Add overall x-axis and y-axis labels\n",
    "fig.supxlabel(f'$\\\\overline{{{dict[x]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.supylabel(f'$\\\\overline{{{dict[y]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.suptitle(f'Flow curves at different values of $\\\\overline{{{dict[y]}}}$ and $\\\\overline{{{dict[x]}}}$', fontsize=15)\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()  # Adjust bottom and top margins #rect=[0, 0, 1, 0.96]\n",
    "# Adjust layout to prevent overlap and increase spacing\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)  # Adjust space between subplots\n",
    "xspace = 0.06\n",
    "yspace = 0.04\n",
    "space = 0.0001\n",
    "plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "# Adding a note using fig.text\n",
    "fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\allgraph3.png', format='png',dpi=300)  # Save as PNG file\n",
    "plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ab0cf-a983-4247-8501-499af3ab856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zl vs PDI_l\n",
    "y = \"Zl\"\n",
    "x = \"PDI_l\"\n",
    "dict = {\"Zs\" : \"Z_s\",\"Zl\" : \"Z_l\",\"PDI_s\" : \"PDI_s\", \"PDI_l\" : \"PDI_l\", \"phiL\" : \"\\phi_l\"}\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "rows, cols = 5, 5\n",
    "\n",
    "# Create a figure with subplots arranged in a grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "\n",
    "# Load and display each image in the grid\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\graph{2}{3}_{0}{1}.png'.format(abs(4 - i),j,y,x)  # Path to the image file\n",
    "        img = mpimg.imread(img_path)  # Load the image\n",
    "        axs[i, j].imshow(img)  # Display the image\n",
    "        axs[i, j].axis('off')  # Hide the axis\n",
    "\n",
    "# Add overall x-axis and y-axis labels\n",
    "fig.supxlabel(f'$\\\\overline{{{dict[x]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.supylabel(f'$\\\\overline{{{dict[y]}}} \\longrightarrow$', fontsize=15)\n",
    "fig.suptitle(f'Flow curves at different values of $\\\\overline{{{dict[y]}}}$ and $\\\\overline{{{dict[x]}}}$', fontsize=15)\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()  # Adjust bottom and top margins #rect=[0, 0, 1, 0.96]\n",
    "# Adjust layout to prevent overlap and increase spacing\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)  # Adjust space between subplots\n",
    "xspace = 0.06\n",
    "yspace = 0.04\n",
    "space = 0.0001\n",
    "plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "# Adding a note using fig.text\n",
    "fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bigraphs\\\\allgraph4.png', format='png',dpi=600)  # Save as PNG file\n",
    "plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f633d-5715-451a-9aa4-13f8a2bd693e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b26fd7-4e5b-4147-a1cd-de5cbd1fda4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedde133-90f4-4e68-8ffb-91b234628093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a068b-5c0a-4d53-8989-321cb34250e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    plt.hist(y[s])\n",
    "    plt.show()\n",
    "    # print(y[s].min())\n",
    "    # print(y[s].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5c918-9917-49f8-bb82-73c860084b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.iloc[:,100:]\n",
    "print(d.head(5))\n",
    "d = d.values.flatten()\n",
    "d\n",
    "print(d.min())\n",
    "print(d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166baa49-9670-412e-a060-f31664a95a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(128, input_shape=(1,190),return_sequences=True)) #return_sequences=True\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(LSTM(16))\n",
    "    model.add(Dense(5, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d532a5-dc43-4994-ad6e-f689e980b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.006788847502321005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772948d8-1a02-4dde-84e1-2272fb69e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18eefe-1d59-4873-b38d-e0cb2b7cc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658099c8-3c13-4928-8c1a-b7861045bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "Zs_mae\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.036031487999376444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c8271-d36f-45ee-9039-d2afb70732b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "Zl_mae\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.026321348207570663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37329e99-feaf-4206-b4b7-95263f8956c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "PDI_s_mae\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.027779294553068122\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862705a4-6064-4c3c-ac40-8bc5f3c8ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "PDI_l_mae\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.02168924099052297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734b801-f65c-4ed4-b15d-2a135d3c6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "phiL_mae\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.0390145898566803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dbfd18-83a4-42a0-96d3-4a759f172c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028b052-4fbd-4d89-a531-e9c4ef71ea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9c275-49a3-4377-893f-308ab5c38b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff86501-8b24-4dbb-a7f9-2344de333cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abf64a-91fa-412f-97f8-1d9f22804a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c77e5e-e1d6-441a-91bc-dfc64fcd8191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c97025-2f41-4c88-9733-b9c2a8e1c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 15 - Error Analysis on Bimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f763ec1-39ac-420f-a637-b9f841bbc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d49ffa-9026-415e-80b1-c56955a503a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77429e6-94b2-4c8d-a281-565cbe9a1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9234e3a-1c20-4add-8998-4a1f8cdf1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 5:]  # 190 explanatory variables\n",
    "y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db03167-d79b-4d2a-88f6-7b4b054f1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal.h5\")\n",
    "\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_reshaped)\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0f6e9-a84a-4f55-8dcc-6ad96d0e2fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ee3d7-37d2-41fd-be59-61d7824bb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.06409748865866961\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.04218965152593366\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.04768009408433895\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.036523118682293854\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.06807153102156327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2670f-000a-45b9-9a2c-d192e7fd9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print({Zs_mae*100/(max(df[\"Zs\"]) - min(df[\"Zs\"]))})\n",
    "print({PDI_s_mae*100/(max(df[\"PDI_s\"]) - min(df[\"PDI_s\"]))})\n",
    "print({Zl_mae*100/(max(df[\"Zl\"]) - min(df[\"Zl\"]))})\n",
    "print({PDI_l_mae*100/(max(df[\"PDI_l\"]) - min(df[\"PDI_l\"]))})\n",
    "print({phiL_mae*100/(max(df[\"phiL\"]) - min(df[\"phiL\"]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e617839-bc86-4001-b654-f91e45bccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(df[\"PDI_lPRED\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693a2e9-17b8-45fe-8fb4-99ea9d8f1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db96340-2994-4a82-8192-53f37c615982",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {min(df[s+\"errors\"]))} , {max(df[s+\"errors\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918a8cf-5348-424c-bcf0-b960b470888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in strings:\n",
    "    print(f'{s} : {len(df[df[s+\"errors\"]>0.06][s+\"errors\"])*100/len(df[s+\"errors\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa116598-ca89-4b46-bd03-cb2560a1361d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbd1f1-2ec3-43e4-b48f-786bba3faf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad4e20-9169-437b-831b-55679118597f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46af40-2a67-44c1-a011-4ffd46ee807d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the points Zs\n",
    "plt.scatter(df[\"Zs\"], df[\"Zserrors\"])\n",
    " \n",
    "# naming the x axis\n",
    "plt.xlabel('Zs')\n",
    "# naming the y axis\n",
    "plt.ylabel('Zs Errors')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('Zs Errors')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()\n",
    "\n",
    "# plotting the points Zl\n",
    "plt.scatter(df[\"Zl\"], df[\"Zlerrors\"])\n",
    " \n",
    "# naming the x axis\n",
    "plt.xlabel('Zl')\n",
    "# naming the y axis\n",
    "plt.ylabel('Zl Errors')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('Zl Errors')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()\n",
    "\n",
    "# plotting the points PDI_s\n",
    "plt.scatter(df[\"PDI_s\"], df[\"PDI_serrors\"])\n",
    " \n",
    "# naming the x axis\n",
    "plt.xlabel('PDI_s')\n",
    "# naming the y axis\n",
    "plt.ylabel('PDI_s Errors')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('PDI_s Errors')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()\n",
    "\n",
    "# plotting the points PDI_l\n",
    "plt.scatter(df[\"PDI_l\"], df[\"PDI_lerrors\"])\n",
    " \n",
    "# naming the x axis\n",
    "plt.xlabel('PDI_l')\n",
    "# naming the y axis\n",
    "plt.ylabel('PDI_l Errors')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('PDI_l Errors')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()\n",
    "\n",
    "# plotting the points \n",
    "plt.scatter(df[\"phiL\"], df[\"phiLerrors\"])\n",
    " \n",
    "# naming the x axis\n",
    "plt.xlabel('phiL')\n",
    "# naming the y axis\n",
    "plt.ylabel('phiL Errors')\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('phiL Errors')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc7546-15c5-4f57-94b6-c44a3a4ade01",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(str(min(df[s])) + \" , \" + str(max(df[s])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c5035-6fc8-4009-b774-728423b7c2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "POINTS = [[]]\n",
    "NEWDATA = [[]]\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "names = [\"Z_s\",\"Z_l\",\"PDI_s\",\"PDI_l\",\"\\phi_l\"]\n",
    "\n",
    "for k,s in enumerate(strings):\n",
    "\n",
    "    for j, S in enumerate(strings):\n",
    "        val = min(df[s])\n",
    "        points = [val]\n",
    "        while val + 0.125 < max(df[s]):\n",
    "            val = val + 0.125\n",
    "            points.append(val)\n",
    "        points.append(max(df[s]))\n",
    "    \n",
    "        #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "        newdata = []\n",
    "        for i,c in enumerate(points[:-1]):\n",
    "            if i!=len(points)-1:\n",
    "                arr = df[df[s]>=points[i]][df[s]<points[i+1]][S+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "            else:\n",
    "                arr = df[df[s]>=points[i]][df[s]<=points[i+1]][S+\"errors\"]\n",
    "                newdata.append(sum(arr)/len(arr))\n",
    "        newdata\n",
    "        \n",
    "        points2 = [x + 0.0625 for x in points[:-1]]\n",
    "        plt.plot(points2, newdata,marker='o', linestyle='-', color='purple')\n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(names[k], names[j]))\n",
    "        plt.show()\n",
    "        #plt.plot(points2, newdata)\n",
    "\n",
    "        #POINTS.append(points2)\n",
    "        #NEWDATA.append(newdata)\n",
    "        \n",
    "        plt.figure(figsize=(7.5, 6))\n",
    "        #plt.figure(figsize=(5, 4))\n",
    "        \n",
    "        # # Plot the density heatmap\n",
    "        # sns.kdeplot(x=df[s], y=df[s+\"errors\"], cmap='Spectral', fill=False, thresh=0, levels=10)\n",
    "        \n",
    "        # # plotting the points \n",
    "        # plt.scatter(df[s], df[s+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "        \n",
    "        # # Plot the density heatmap\n",
    "        # #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "        # plt.plot(points2, newdata)\n",
    "    \n",
    "        median_x = np.median(df[s])\n",
    "        median_y = np.median(df[S+\"errors\"])\n",
    "        \n",
    "        Q1_x, Q3_x = np.percentile(df[s], [25, 75])\n",
    "        Q1_y, Q3_y = np.percentile(df[S+\"errors\"], [25, 75])\n",
    "        \n",
    "        IQR_x = Q3_x - Q1_x\n",
    "        IQR_y = Q3_y - Q1_y\n",
    "        # Plot the density heatmap\n",
    "        sns.kdeplot(x=df[s], y=df[S+\"errors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "        \n",
    "        # plotting the points \n",
    "        plt.scatter(df[s], df[S+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "        \n",
    "        plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "        # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "        plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "        plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "        # Plot the density heatmap\n",
    "        #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "        plt.plot(points2, newdata,marker='o', linestyle='-', color='lime')\n",
    "        plt.xlim(df[s].min()-0.3, df[s].max()+0.3)  # Set the y-axis limit from 0 to 1\n",
    "        #plt.ylim(0, 0.06)  # Set the y-axis limit from 0 to 1\n",
    "        \n",
    "        # naming the x axis\n",
    "        plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "         \n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{{{0}}} \\text{{ Errors with Density Heatmap }}$'.format(names[j]))\n",
    "         \n",
    "        # function to show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce752a-d491-432b-a58d-7b25e8624240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code2():\n",
    "    POINTS = [[]]\n",
    "    NEWDATA = [[]]\n",
    "    strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "    names = [\"Z_s\",\"Z_l\",\"PDI_s\",\"PDI_l\",\"\\phi_l\"]\n",
    "    \n",
    "    for k,s in enumerate(strings):\n",
    "    \n",
    "        for j, S in enumerate(strings):\n",
    "            val = min(df[s])\n",
    "            points = [val]\n",
    "            while val + 0.125 < max(df[s]):\n",
    "                val = val + 0.125\n",
    "                points.append(val)\n",
    "            points.append(max(df[s]))\n",
    "        \n",
    "            #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "            newdata = []\n",
    "            for i,c in enumerate(points[:-1]):\n",
    "                if i!=len(points)-1:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "                else:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<=points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "            newdata\n",
    "            \n",
    "            points2 = [x + 0.0625 for x in points[:-1]]\n",
    "            plt.plot(points2, newdata,marker='o', linestyle='-', color='purple')\n",
    "            # naming the x axis\n",
    "            plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "            # naming the y axis\n",
    "            plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "             \n",
    "            # giving a title to my graph\n",
    "            plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(names[k], names[j]))\n",
    "            plt.show()\n",
    "            #plt.plot(points2, newdata)\n",
    "    \n",
    "            #POINTS.append(points2)\n",
    "            #NEWDATA.append(newdata)\n",
    "            \n",
    "            plt.figure(figsize=(7.5, 6))\n",
    "            #plt.figure(figsize=(5, 4))\n",
    "            \n",
    "            # # Plot the density heatmap\n",
    "            # sns.kdeplot(x=df[s], y=df[s+\"errors\"], cmap='Spectral', fill=False, thresh=0, levels=10)\n",
    "            \n",
    "            # # plotting the points \n",
    "            # plt.scatter(df[s], df[s+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "            \n",
    "            # # Plot the density heatmap\n",
    "            # #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "            # plt.plot(points2, newdata)\n",
    "        \n",
    "            median_x = np.median(df[s])\n",
    "            median_y = np.median(df[S+\"errors\"])\n",
    "            \n",
    "            Q1_x, Q3_x = np.percentile(df[s], [25, 75])\n",
    "            Q1_y, Q3_y = np.percentile(df[S+\"errors\"], [25, 75])\n",
    "            \n",
    "            IQR_x = Q3_x - Q1_x\n",
    "            IQR_y = Q3_y - Q1_y\n",
    "            # Plot the density heatmap\n",
    "            sns.kdeplot(x=df[s], y=df[S+\"errors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "            \n",
    "            # plotting the points \n",
    "            plt.scatter(df[s], df[S+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "            \n",
    "            plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "            # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "            plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "            plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "            # Plot the density heatmap\n",
    "            #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "            plt.plot(points2, newdata,marker='o', linestyle='-', color='lime')\n",
    "            plt.xlim(df[s].min()-0.3, df[s].max()+0.3)  # Set the y-axis limit from 0 to 1\n",
    "            #plt.ylim(0, 0.06)  # Set the y-axis limit from 0 to 1\n",
    "            \n",
    "            # naming the x axis\n",
    "            plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "            # naming the y axis\n",
    "            plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "             \n",
    "            # giving a title to my graph\n",
    "            plt.title(r'$\\overline{{{0}}} \\text{{ Errors with Density Heatmap }}$'.format(names[j]))\n",
    "             \n",
    "            # function to show the plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4a73a-5899-41e8-aaac-e02fab7d88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code3(bestname):\n",
    "    POINTS = [[]]\n",
    "    NEWDATA = [[]]\n",
    "    strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "    names = [\"Z_s\",\"Z_l\",\"PDI_s\",\"PDI_l\",\"\\phi_l\"]\n",
    "    \n",
    "    for k,s in enumerate(strings):\n",
    "    \n",
    "        for j, S in enumerate(strings):\n",
    "            val = min(df[s])\n",
    "            points = [val]\n",
    "            while val + 0.125 < max(df[s]):\n",
    "                val = val + 0.125\n",
    "                points.append(val)\n",
    "            points.append(max(df[s]))\n",
    "        \n",
    "            #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "            newdata = []\n",
    "            for i,c in enumerate(points[:-1]):\n",
    "                if i!=len(points)-1:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "                else:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<=points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "            newdata\n",
    "            \n",
    "            points2 = [x + 0.0625 for x in points[:-1]]\n",
    "            plt.plot(points2, newdata,marker='o', linestyle='-', color='purple')\n",
    "            # naming the x axis\n",
    "            plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "            # naming the y axis\n",
    "            plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "             \n",
    "            # giving a title to my graph\n",
    "            plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(names[k], names[j]))\n",
    "            plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\binormal\\\\{2}_graph_{0}{1}.png'.format(k,j,bestname), format='png',dpi=600)  # Save as PNG file\n",
    "            plt.show()\n",
    "            \n",
    "            # plt.figure(figsize=(7.5, 6))\n",
    "        \n",
    "            # median_x = np.median(df[s])\n",
    "            # median_y = np.median(df[S+\"errors\"])\n",
    "            \n",
    "            # Q1_x, Q3_x = np.percentile(df[s], [25, 75])\n",
    "            # Q1_y, Q3_y = np.percentile(df[S+\"errors\"], [25, 75])\n",
    "            \n",
    "            # IQR_x = Q3_x - Q1_x\n",
    "            # IQR_y = Q3_y - Q1_y\n",
    "            # # Plot the density heatmap\n",
    "            # sns.kdeplot(x=df[s], y=df[S+\"errors\"], cmap='Spectral', fill=False, thresh=0.05, levels=2)\n",
    "            \n",
    "            # # plotting the points \n",
    "            # plt.scatter(df[s], df[S+\"errors\"],s=0.1, c='black', alpha=0.5)\n",
    "            \n",
    "            # plt.axhline(median_y, color='red', linestyle='--', linewidth=0.5, label='Median Y')\n",
    "            # # plt.axvline(median_x, color='red', linestyle='--', linewidth=0.5, label='Median X')\n",
    "            # plt.axhline(Q1_y, color='red', linestyle='--', linewidth=0.5, label='1st Quartile Y')\n",
    "            # plt.axhline(Q3_y, color='red', linestyle='--', linewidth=0.5, label='3rd Quartile Y')\n",
    "            # # Plot the density heatmap\n",
    "            # #sns.kdeplot(x=df[\"Z\"], y=df[\"zerrors\"], cmap=\"Blues\", fill=True, thresh=0, levels=100)\n",
    "            # plt.plot(points2, newdata,marker='o', linestyle='-', color='lime')\n",
    "            # plt.xlim(df[s].min()-0.3, df[s].max()+0.3)  # Set the y-axis limit from 0 to 1\n",
    "            # #plt.ylim(0, 0.06)  # Set the y-axis limit from 0 to 1\n",
    "            \n",
    "            # # naming the x axis\n",
    "            # plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "            # # naming the y axis\n",
    "            # plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "             \n",
    "            # # giving a title to my graph\n",
    "            # plt.title(r'$\\overline{{{0}}} \\text{{ Errors with Density Heatmap }}$'.format(names[j]))\n",
    "             \n",
    "            # # function to show the plot\n",
    "            # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fb9d8-14ef-405b-9bca-6ccd2ea13f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code3(\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617994d-589e-40cd-a584-437938b741c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "def code4(bestname):\n",
    "    \n",
    "    # Define the number of rows and columns for the grid\n",
    "    rows, cols = 5, 5\n",
    "    \n",
    "    # Create a figure with subplots arranged in a grid\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "    \n",
    "    # Define the labels for rows and columns\n",
    "    labels = [r\"$\\overline{\\phi_l}$\", r\"$\\overline{PDI_l}$\", r\"$\\overline{PDI_s}$\", r\"$\\overline{Z_l}$\",r\"$\\overline{Z_s}$\"]\n",
    "    \n",
    "    # Load and display each image in the grid\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            img_path = 'C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\binormal\\\\{2}_graph_{0}{1}.png'.format(j, abs(4 - i), bestname)  # Path to the image file\n",
    "            img = mpimg.imread(img_path)  # Load the image\n",
    "            axs[i, j].imshow(img)  # Display the image\n",
    "            axs[i, j].axis('off')  # Hide the axis\n",
    "    \n",
    "    # Add row and column labels\n",
    "    for i, label in enumerate(labels):\n",
    "        # Add labels to the left of each row\n",
    "        fig.text(0.04, 0.87 - i * 0.1785, label, va='center', ha='center', fontsize=12, rotation='horizontal')\n",
    "        # Add labels above each column\n",
    "        fig.text(0.15 + i * 0.178, 0.037, labels[len(labels)-1-i], va='center', ha='center', fontsize=12) #0.92\n",
    "    # # Add overall x-axis and y-axis labels\n",
    "    # fig.supxlabel(r'$\\overline{PDI} \\longrightarrow$', fontsize=15)\n",
    "    # fig.supylabel(r'$\\overline{Z} \\longrightarrow$', fontsize=15)\n",
    "    # fig.suptitle(f'Flow curves at different values of $\\\\overline{{Z}}$ and $\\\\overline{{PDI}}$', fontsize=15)\n",
    "    \n",
    "    # Adjust layout to prevent overlap and increase spacing\n",
    "    xspace = 0.06\n",
    "    yspace = 0.04\n",
    "    space = 0.0001\n",
    "    plt.subplots_adjust(left=xspace, right=1-xspace, top=1-yspace, bottom=yspace, wspace=space, hspace=space)\n",
    "    \n",
    "    # Adding a note using fig.text\n",
    "    # fig.text(0.79, 0.01, f\"(Note: Red curve denotes $\\\\overline{{G'}}$ and Blue curve denotes $\\\\overline{{G''}})$\", ha='center', fontsize=9)\n",
    "    \n",
    "    # Save and show the plot\n",
    "    plt.savefig('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\binormal\\\\allgraph.png', format='png', dpi=600)  # Save as PNG file\n",
    "    plt.show()  # Display the grid of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f2084-5d51-42ad-8392-aed1c5a55353",
   "metadata": {},
   "outputs": [],
   "source": [
    "code4(\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a72f9-802d-4640-9515-030e5af73590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code5(bestthing):\n",
    "    code3(bestthing)\n",
    "    code4(bestthing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37d8d-736a-4ab7-9202-77f574813d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee046e3-7c26-40da-8a9a-20e2cf804911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac021b2-53e3-4e3f-9a4e-fe8a11fda443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 16 - Modelling bimodal with restricted freq range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83798bdb-e9b5-4a6a-987e-4b9b46caa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path\n",
    "\n",
    "#X = data.iloc[:, 5:]  # 190 explanatory variables\n",
    "\n",
    "X1 = data.iloc[:, 22+3:49+3]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = data.iloc[:, 117+3:144+3]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb20453-00dc-4fbd-a138-a4033157092e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(128, input_shape=(1,54),return_sequences=True)) #return_sequences=True\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(LSTM(16))\n",
    "    model.add(Dense(5, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=128, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c9413-21aa-4179-bc7b-d278ac7c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.006788847502321005\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.036031487999376444\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.026321348207570663\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.027779294553068122\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.02168924099052297\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.0390145898566803\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708ad59-2282-44ce-b070-de1f5284077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal_restricted.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_restricted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a09e0-f5c7-49a7-8d86-e1eb12ded6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd107d20-becd-4a7c-9291-8339dc2be670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 17 - Error analysis of Restricted Bimodal thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df06be-fc46-4ca3-ad02-10bc9ab82848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_restricted.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d3f31-3810-4f10-b189-2c1face629e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12749466-ec6a-4d48-9f76-c02d61a609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "code5(\"restricted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651e9c9-6fd1-4483-8f80-9a7e037ab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(str(min(df[s])) + \" , \" + str(max(df[s])))\n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.06409748865866961\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.04218965152593366\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.04768009408433895\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.036523118682293854\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.06807153102156327\n",
    "\n",
    "print({Zs_mae*100/(max(df[\"Zs\"]) - min(df[\"Zs\"]))})\n",
    "print({PDI_s_mae*100/(max(df[\"PDI_s\"]) - min(df[\"PDI_s\"]))})\n",
    "print({Zl_mae*100/(max(df[\"Zl\"]) - min(df[\"Zl\"]))})\n",
    "print({PDI_l_mae*100/(max(df[\"PDI_l\"]) - min(df[\"PDI_l\"]))})\n",
    "print({phiL_mae*100/(max(df[\"phiL\"]) - min(df[\"phiL\"]))})\n",
    "\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {min(df[s+\"errors\"])} , {max(df[s+\"errors\"])}')\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {len(df[df[s+\"errors\"]>0.06][s+\"errors\"])*100/len(df[s+\"errors\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c39b6-f204-4e90-8e3b-406875ef9327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dacd4c-0c9b-41f8-9541-15acdf86fed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ca4ff-964c-441b-95ab-26c0e9f1473c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0c9da-48b7-40ba-935e-4eab50fdba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312493c-747c-4642-bbf5-7011c9f4de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 18 - Modelling bimodal with inducing artificial errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89b4ae-5a3e-4360-ae93-31865f685fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path\n",
    "\n",
    "X = data.iloc[:, 5:]  # 190 explanatory variables\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (408050, 1))\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446931b7-5b4e-4795-92af-cfff7a25bb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(128, input_shape=(1,190),return_sequences=True)) #return_sequences=True\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(LSTM(16))\n",
    "    model.add(Dense(5, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=128, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39c679-fdf2-4c2c-a41f-428a39827da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.006788847502321005\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.036031487999376444\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.026321348207570663\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.027779294553068122\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.02168924099052297\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.0390145898566803\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28618f-a6d7-42b7-b9e1-72a198a6882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal_randomerrors.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d6d97-378d-4de3-a980-966702094c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b910e-2304-46e8-a21d-d1b2470a691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 19 - Error analysis of Random error prediction bimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130636b-07be-473a-9ca3-739ae110b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f121035-7d9a-4e4d-a83e-f33bdaec0f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f58ef0-89b2-40f2-8343-2d7fea2cd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(str(min(df[s])) + \" , \" + str(max(df[s])))\n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.06409748865866961\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.04218965152593366\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.04768009408433895\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.036523118682293854\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.06807153102156327\n",
    "\n",
    "print({Zs_mae*100/(max(df[\"Zs\"]) - min(df[\"Zs\"]))})\n",
    "print({PDI_s_mae*100/(max(df[\"PDI_s\"]) - min(df[\"PDI_s\"]))})\n",
    "print({Zl_mae*100/(max(df[\"Zl\"]) - min(df[\"Zl\"]))})\n",
    "print({PDI_l_mae*100/(max(df[\"PDI_l\"]) - min(df[\"PDI_l\"]))})\n",
    "print({phiL_mae*100/(max(df[\"phiL\"]) - min(df[\"phiL\"]))})\n",
    "\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {min(df[s+\"errors\"])} , {max(df[s+\"errors\"])}')\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {len(df[df[s+\"errors\"]>0.06][s+\"errors\"])*100/len(df[s+\"errors\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd24fa-4451-41e2-aaad-7d9578cb3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code5(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95448250-8dd1-4507-ad59-0957211daa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2c89f-6470-4cc0-8dac-c8a49be174d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9ed20-6d2f-4bb4-a2ed-202dd44beca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af00f2c-c829-48d6-8bbb-8b6280918f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 20 - Modelling bimodal with pseudo real life data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08ac66-a3a3-4a16-8920-e264aff7212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path\n",
    "\n",
    "X1 = data.iloc[:, 22+3:49+3]  # (G'(20) to G'(46)) (10^-1 to 10^2) (27 columns)\n",
    "X2 = data.iloc[:, 117+3:144+3]  # (G''(20) to G''(46)) (10^-1 to 10^2) (27 columns)\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "\n",
    "noise = np.random.normal(0, 0.1, (408050, 1))\n",
    "X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "noise = np.random.normal(0, 0.05, X.shape)\n",
    "X = X + noise #additive errors (element wise)\n",
    "\n",
    "y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xcopytest = X_test\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_trainNP = X_train.flatten()\n",
    "y_trainNP = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd13c64-58fc-4fd5-9d16-f22cb16f4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=['mae']\n",
    "#Define the MLP model\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(LSTM(128, input_shape=(1,54),return_sequences=True)) #return_sequences=True\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.05))\n",
    "    #model.add(LSTM(16))\n",
    "    model.add(Dense(5, activation='linear'))  # Output layer for 2 numerical variables\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_mlp_model((1,X_train.shape[1]))\n",
    "\n",
    "#Create the Adam optimizer with a custom learning rate\n",
    "learning_rate = 0.0001  # Set your desired learning rate here #10^-4 before\n",
    "custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error', metrics=['accuracy'])#metrics=['mae']\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_trainNP, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=128, \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "#Test Loss: 9.01788444025442e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae9346-90f8-45fb-9201-2ef0820147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "loss = model.evaluate(X_test_reshaped, y_test.to_numpy())\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "#Best Test Loss: 0.006788847502321005\n",
    "\n",
    "#df1 = pd.DataFrame(xcopytest,columns = data.columns[2:])\n",
    "df1 = xcopytest.reset_index(drop=False)\n",
    "df1[\"index\"] = df1[\"index\"] + 2\n",
    "df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "df3 = pd.DataFrame(y_pred,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "df = df1.join(df2).join(df3)   \n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.036031487999376444\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.026321348207570663\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.027779294553068122\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.02168924099052297\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.0390145898566803\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3200ef-892d-4431-bc4a-ec4269f3c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model.save(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal_realdata.h5\")\n",
    "# df.to_csv('C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_realdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52089cd0-0b22-44ac-806e-36eb54d02a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e68d6-92ef-4246-86e4-5c1b5b244b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 21 - Error analysis of bimodal pseudo real life data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72244bc6-c8a4-44fc-8fab-67457fd0ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_realdata.csv\")  # Replace with your CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cfca2-852e-4d9b-a44e-76a05fd3f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773036c4-742f-4b35-b395-15b520f5823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(str(min(df[s])) + \" , \" + str(max(df[s])))\n",
    "\n",
    "df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "print(Zs_mae)\n",
    "\n",
    "#Best Zs_mae (RNN) = 0.06409748865866961\n",
    "\n",
    "df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "print(Zl_mae)\n",
    "\n",
    "#Best Zl_mae (RNN) = 0.04218965152593366\n",
    "\n",
    "df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "print(PDI_s_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.04768009408433895\n",
    "\n",
    "df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "print(PDI_l_mae)\n",
    "\n",
    "#Best PDI_s_mae (RNN) = 0.036523118682293854\n",
    "\n",
    "df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "print(phiL_mae)\n",
    "\n",
    "#Best phiL_mae (RNN) = 0.06807153102156327\n",
    "\n",
    "print({Zs_mae*100/(max(df[\"Zs\"]) - min(df[\"Zs\"]))})\n",
    "print({PDI_s_mae*100/(max(df[\"PDI_s\"]) - min(df[\"PDI_s\"]))})\n",
    "print({Zl_mae*100/(max(df[\"Zl\"]) - min(df[\"Zl\"]))})\n",
    "print({PDI_l_mae*100/(max(df[\"PDI_l\"]) - min(df[\"PDI_l\"]))})\n",
    "print({phiL_mae*100/(max(df[\"phiL\"]) - min(df[\"phiL\"]))})\n",
    "\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {min(df[s+\"errors\"])} , {max(df[s+\"errors\"])}')\n",
    "\n",
    "for s in strings:\n",
    "    print(f'{s} : {len(df[df[s+\"errors\"]>0.06][s+\"errors\"])*100/len(df[s+\"errors\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa85f7-2aac-4bd1-bc12-44dd8d1847e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code5(\"pseudo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9940c-a15c-4604-b427-5328bc4ceaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f9d68-3272-4fa8-b8f5-7e46b590b148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fa357-f56a-4875-bf4c-7f74e2e97cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe7116-c9c6-44d9-b8e1-f5b21b70d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the graphs together - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e4a65-4469-499a-93d9-94e9f6d0cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8fde9-28c9-4cf0-9085-eb45717d8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = [\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_restricted.csv\",\n",
    "            \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\",\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_realdata.csv\"]\n",
    "\n",
    "POINTS = [[]]\n",
    "NEWDATA = [[]]\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "for l in loadings:\n",
    "    df = pd.read_csv(l)  # Replace with your CSV file path\n",
    "    for i,s in enumerate(strings):\n",
    "    \n",
    "        for j, S in enumerate(strings):\n",
    "            val = min(df[s])\n",
    "            points = [val]\n",
    "            while val + 0.125 < max(df[s]):\n",
    "                val = val + 0.125\n",
    "                points.append(val)\n",
    "            points.append(max(df[s]))\n",
    "        \n",
    "            #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "            newdata = []\n",
    "            for i,c in enumerate(points[:-1]):\n",
    "                if i!=len(points)-1:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "                else:\n",
    "                    arr = df[df[s]>=points[i]][df[s]<=points[i+1]][S+\"errors\"]\n",
    "                    newdata.append(sum(arr)/len(arr))\n",
    "            newdata\n",
    "            \n",
    "            points2 = [x + 0.0625 for x in points[:-1]]\n",
    "            # plt.plot(points2, newdata,marker='o', linestyle='-', color='purple')\n",
    "            # plt.title(S +' Errors'+' vs ' +s)\n",
    "            # plt.xlabel(s)\n",
    "            # plt.ylabel(S +' Errors')\n",
    "            # plt.show()\n",
    "            # #plt.plot(points2, newdata)\n",
    "    \n",
    "            POINTS.append(points2)\n",
    "            NEWDATA.append(newdata)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bcfff-e59a-4078-9792-5909673ad9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every combination\n",
    "count = 0\n",
    "strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "names = [\"Z_s\",\"Z_l\",\"PDI_s\",\"PDI_l\",\"\\phi_l\"]\n",
    "for i,s in enumerate(strings): \n",
    "    for j, S in enumerate(strings):\n",
    "        plt.plot(POINTS[76+count], NEWDATA[76+count],marker='o', linestyle='-', color='purple',label='Prediction with restricted frequency and artificial errors induced (Pseudo real life data)')\n",
    "        plt.plot(POINTS[51+count], NEWDATA[51+count],marker='o', linestyle='-', color='red',label='Prediction with artificial errors induced')\n",
    "        plt.plot(POINTS[26+count], NEWDATA[26+count],marker='o', linestyle='-', color='b',label='Prediction with restricted frequency range')\n",
    "        plt.plot(POINTS[1+count], NEWDATA[1+count],marker='o', linestyle='-', color='lime',label='Prediction on original dataset')\n",
    "        plt.xlabel(r'$\\overline{{{0}}}$'.format(names[i]))\n",
    "        # naming the y axis\n",
    "        plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[j]))\n",
    "        # giving a title to my graph\n",
    "        plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(names[i], names[j]))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec30746-fabe-4f3a-91ae-1e8db8a22cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618386a4-d327-41e0-85c8-743cba61f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code 22 - Training on dirty data and predicting on clean and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed5303-560c-4fa5-9e63-f8c0c9eb0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "def bitrying(moodel, deteset, texts):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\bi_PE_5param_wint_2_400000_training.csv\")  # Replace with your CSV file path\n",
    "    #data[\" PDI\"] = np.log10((((10 ** data[\" PDI\"]) + 1)*1))\n",
    "    \n",
    "    X = data.iloc[:, 5:]  # 190 explanatory variables\n",
    "    y = data.iloc[:, :5]  # 2 numerical variables to predict\n",
    "\n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\":\n",
    "        noise = np.random.normal(0, 0.1, (408050, 1))\n",
    "        # noise2 = np.random.normal(1, 0, (202500, 1))\n",
    "        # noise3 = noise + noise2\n",
    "        X = X.add(noise, axis=0) #multiplicative errors (row - wise)\n",
    "        noise = np.random.normal(0, 0.05, X.shape)\n",
    "        X = X + noise #additive errors (element wise)\n",
    "        \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    xcopytest = X_test\n",
    "    \n",
    "    # Standardize the data (mean=0, variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #X_trainNP = X_train.flatten()\n",
    "    y_trainNP = y_train.to_numpy()\n",
    "    \n",
    "    # Load the model\n",
    "    loaded_model = tf.keras.models.load_model(moodel)\n",
    "    \n",
    "    X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    # Make predictions with the loaded model\n",
    "    y_pred_loaded = loaded_model.predict(X_test_reshaped)\n",
    "    \n",
    "    df1 = pd.DataFrame(xcopytest,columns = X.columns)\n",
    "    df1 = df1.reset_index(drop=False)\n",
    "    df1[\"index\"] = df1[\"index\"] + 2\n",
    "    df2 = pd.DataFrame(y_test.to_numpy(),columns = [x for x in y_test.columns])\n",
    "    df3 = pd.DataFrame(y_pred_loaded,columns = [x+\"PRED\" for x in y_test.columns])\n",
    "    df = df1.join(df2).join(df3)\n",
    "\n",
    "    df[\"Zserrors\"] = (df[\"Zs\"] - df[\"ZsPRED\"]).abs()\n",
    "    Zs_mae =sum(df[\"Zserrors\"])/len(df[\"Zserrors\"])\n",
    "    print(Zs_mae)\n",
    "    \n",
    "    #Best Zs_mae (RNN) = 0.036031487999376444\n",
    "    \n",
    "    df[\"Zlerrors\"] = (df[\"Zl\"] - df[\"ZlPRED\"]).abs()\n",
    "    Zl_mae =sum(df[\"Zlerrors\"])/len(df[\"Zlerrors\"])\n",
    "    print(Zl_mae)\n",
    "    \n",
    "    #Best Zl_mae (RNN) = 0.026321348207570663\n",
    "    \n",
    "    df[\"PDI_serrors\"] = (df[\"PDI_s\"] - df[\"PDI_sPRED\"]).abs()\n",
    "    PDI_s_mae =sum(df[\"PDI_serrors\"])/len(df[\"PDI_serrors\"])\n",
    "    print(PDI_s_mae)\n",
    "    \n",
    "    #Best PDI_s_mae (RNN) = 0.027779294553068122\n",
    "    \n",
    "    df[\"PDI_lerrors\"] = (df[\"PDI_l\"] - df[\"PDI_lPRED\"]).abs()\n",
    "    PDI_l_mae =sum(df[\"PDI_lerrors\"])/len(df[\"PDI_lerrors\"])\n",
    "    print(PDI_l_mae)\n",
    "    \n",
    "    #Best PDI_s_mae (RNN) = 0.02168924099052297\n",
    "    \n",
    "    df[\"phiLerrors\"] = (df[\"phiL\"] - df[\"phiLPRED\"]).abs()\n",
    "    phiL_mae =sum(df[\"phiLerrors\"])/len(df[\"phiLerrors\"])\n",
    "    print(phiL_mae)\n",
    "\n",
    "    #Best phiL_mae (RNN) = 0.0390145898566803\n",
    "    \n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\":\n",
    "        deteset2 = \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal.csv\"\n",
    "    elif deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal.csv\":\n",
    "        deteset2 = \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\"\n",
    "\n",
    "    loadings = [0,deteset2]\n",
    "    count = 0\n",
    "    NEWDATA = [[]]\n",
    "    POINTS = [[]]\n",
    "    strings = [\"Zs\",\"Zl\",\"PDI_s\",\"PDI_l\",\"phiL\"]\n",
    "    names = [\"Z_s\",\"Z_l\",\"PDI_s\",\"PDI_l\",\"\\phi_l\"]\n",
    "    for l in loadings:\n",
    "        if l!=0:\n",
    "            df = pd.read_csv(l)\n",
    "        for i,s in enumerate(strings):\n",
    "        \n",
    "                for j,S in enumerate(strings):\n",
    "                    val = min(df[s])\n",
    "                    points = [val]\n",
    "                    while val + 0.0625*2 < max(df[s]):\n",
    "                        val = val + 0.0625*2\n",
    "                        points.append(val)\n",
    "                    points.append(max(df[s]))\n",
    "                \n",
    "                    #points = [min(df[\" PDI\"]),-1,-0.5,0,0.5,max(df[\" PDI\"])]\n",
    "                    newdata = []\n",
    "                    for i,c in enumerate(points[:-1]):\n",
    "                        if i!=len(points)-1:\n",
    "                            arr = df[df[s]>=points[i]][df[s]<points[i+1]][S+\"errors\"]\n",
    "                            newdata.append(sum(arr)/len(arr))\n",
    "                        else:\n",
    "                            arr = df[df[s]>=points[i]][df[s]<=points[i+1]][S+\"errors\"]\n",
    "                            newdata.append(sum(arr)/len(arr))\n",
    "                    newdata\n",
    "                    #newdataZ = newdata\n",
    "                    points = [x + 0.0625 for x in points[:-1]]\n",
    "                    POINTS.append(points)\n",
    "                    NEWDATA.append(newdata)\n",
    "                    #plt.plot(points, newdata,marker='o', linestyle='-', color='lime')\n",
    "                    #plt.show()\n",
    "        \n",
    "    if deteset == \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\":\n",
    "        extra = 0\n",
    "    else:\n",
    "        extra = 2\n",
    "\n",
    "    for k,ss in enumerate(strings):\n",
    "        for kk,sss in enumerate(strings):\n",
    "            plt.plot(POINTS[1+k*kk], NEWDATA[1+k*kk],marker='o', linestyle='-', color='red',label=texts[0+extra])\n",
    "            plt.plot(POINTS[26+k*kk], NEWDATA[26+k*kk],marker='o', linestyle='-', color='lime',label=texts[1+extra])\n",
    "            plt.title(sss+' Errors vs '+ss)\n",
    "            plt.xlabel(ss)\n",
    "            plt.ylabel(sss+' Errors')\n",
    "            plt.xlabel(r'$\\overline{{{0}}}$'.format(names[k]))\n",
    "            # naming the y axis\n",
    "            plt.ylabel(r'$\\overline{{{0}}} \\text{{ Errors}}$'.format(names[kk]))\n",
    "            # giving a title to my graph\n",
    "            plt.title(r'$\\overline{{{1}}} \\text{{ Errors vs }} \\overline{{{0}}}$'.format(names[k], names[kk]))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9d19f-8443-4e54-9033-6bc5df559e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "textss = [\"Prediction of clean model on unclean data\",\"Prediction of clean model on clean data\",\"Prediction of unclean model on clean data\",\"Prediction of unclean model on unclean data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdac5a7-bcf4-4848-8906-7cf225134f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitrying(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal.h5\", \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal_randomerrors.csv\",textss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0331b74-8f6b-4f81-85c2-89c22ad36b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitrying(\"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\mlp_model_binormal_randomerrors.h5\", \"C:\\\\Users\\\\Kevin\\\\Desktop\\\\Dissertation\\\\out_binormal.csv\",textss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1db98-24bf-4b84-bd34-b841d0c4970b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a22caf-1a1e-453b-93e9-b0789468db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hopefully the end :_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983189c7-db83-4af9-b1a9-35519fb37c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
