{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## COMP5712M: Programming for Data Science\n",
    "\n",
    "\n",
    "## Assignment 2: Cities and Earthquakes\n",
    "\n",
    "### @author: Kevin Timothy Muller\n",
    "#### Last Modified: 27th September 2023\n",
    "\n",
    "### Do not import any other module besides those already provided in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: World Cities\n",
    "\n",
    "In this coursework exercise, you will download data from the provided link and read it in as a `CSV` file using the Pandas data analysis package for Python.\n",
    "The data we will use contains a variety of information about cities from around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Techniques for Working with `DataFrame`s\n",
    "\n",
    "To complete these tasks you will need to access and filter a `DataFrame`. \n",
    "The `DataFrame` data structure has many convenient features for extracting and ordering information. Although conceptually it can be thought of as a comptuational\n",
    "represention of a table, it is quite a complex data structure and takes a while\n",
    "to master. The following questions can be done with only a small but powerful\n",
    "set of `DataFrame` operations; and the following examples of typical forms of programming with `DataFrame`s should be useful \n",
    "for coding your answers.\n",
    "\n",
    "#### Loading data from a CSV file into a `DataFrame`\n",
    "\n",
    "`DataFrame`s are specifically designed to handle data organised in a tabular format.\n",
    "Hence, as we would expect, since `CSV` is the standard format for tabular data, it is very easy to \n",
    "create a `DataFrame` by loading data from a `CSV` file.\n",
    "\n",
    "* ```pandas.read_csv(source)``` ---\n",
    "The  [read_csv function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv) can accept a filename or URL as an argument.\n",
    "\n",
    "#### Getting the Data for this Question\n",
    "\n",
    "Download the data file ```worldcities.csv``` from Minerva, and put\n",
    "the file in the same directory as this Jupyter notebook file.\n",
    "Then, by running the following cell, we can set the \n",
    "global variable ```WC_DF``` to a DataFrame containing the information from ```worldcities.csv```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WC_DF Initialisation\n",
    "\n",
    "import pandas  ## This is the module for creating and manipulating DataFrames\n",
    "\n",
    "WC_DF = pandas.read_csv(\"worldcities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be sure to keep the same variable name** `WC_DF` **for this global variable, otherwise most of the\n",
    "following code will not work and you will break the autograder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the contents of a DataFrame\n",
    "\n",
    "Pandas provides the following useful methods that enable you to quickly check the contents of a `DataFrame`:\n",
    "\n",
    "* ```df.head()``` ---\n",
    "  For a DataFrame object, ```df```, this method extracts the first 5 rows of data, so you can easily check what the data looks like.\n",
    "  \n",
    "* ```df.describe()``` --- for a DataFrame, ```df```, this method provides a table giving and overview of some basic statistical properties of the DataFrame.\n",
    "\n",
    "Note that the ```head()``` and ```describe()``` methods are actually operations that \n",
    "return a new DataFrame object. If this value is returned by the last line of a cell\n",
    "it will be displayed as a table, but if it is generated elsewhere in the code\n",
    "you will not see any output unless you use the ```display``` function from the\n",
    "```IPython.display``` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing DataFrame columns and rows\n",
    "\n",
    "Each column of a `DataFrame` is a list-like object called a\n",
    "`Series`. Elements, and slices of a `Series` can then be accessed in similar\n",
    "fashion to a list. The following illustrates how get the `Series` containing\n",
    "the first 5 elements of the `city` column of `WC_DF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Tokyo\n",
       "1       New York\n",
       "2    Mexico City\n",
       "3         Mumbai\n",
       "4      São Paulo\n",
       "Name: city, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_cities = WC_DF[\"city\"][:5]   ## selects the first 5 items of the \"city\" column.\n",
    "top_5_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, the left hand column of the displayed value of `top_5_cities` \n",
    "shows the index label of each element. One of the differences between a `Series` and an ordinary list is that, whereas a list always has integers for its index labels, a `Series` can have different kinds of values for these. For instance (though there\n",
    "is no reason to do this for the current assignment) we could set the index values to alphabetic letters, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a          Tokyo\n",
       "b       New York\n",
       "c    Mexico City\n",
       "d         Mumbai\n",
       "e      São Paulo\n",
       "Name: city, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_cities.index = list(\"abcde\")\n",
    "top_5_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a', 'b', 'c', 'd', 'e'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_cities.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use ```.values``` to return an `array` of the column values without\n",
    "the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tokyo', 'New York', 'Mexico City', 'Mumbai', 'São Paulo'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC_DF[\"city\"][:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `array` is also a list-like datastructure. It does not have an index. The main difference between a list and an `array` is that the list is optimised for\n",
    "storing large amounts of information and for efficiently applying numerical and\n",
    "other operations to all elements of the array. Hence, `array`s are usually preferred\n",
    "to lists when handling large amounts of information, or when storing numerical\n",
    "vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily find the column names of the DataFrame using ```.columns```, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'city_ascii', 'lat', 'lng', 'country', 'iso2', 'iso3',\n",
       "       'admin_name', 'capital', 'population', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC_DF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The `Index` returned here is yet another type of list-like, object. It is similar to an array,\n",
    "except that it is used for indexing a `Series` or `DataFrame`. You do not usually\n",
    "need to create or deal with `Index` objects directly, since this is done automatically when you create and minipulate `DataFrame`s. So you will normally only see one, when\n",
    "you want to look at the columns or rows of a `DataFrame`. But what you should be\n",
    "aware of, when dealing with `DataFrames`, is that the word _index_ can refer to\n",
    "several different types of thing.\n",
    "\n",
    "In many cases you can treat `Series`, `array` and `Index` objects like lists and if you want to change them to an ordinary list you can just use the `list` operator,\n",
    "as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'city_ascii',\n",
       " 'lat',\n",
       " 'lng',\n",
       " 'country',\n",
       " 'iso2',\n",
       " 'iso3',\n",
       " 'admin_name',\n",
       " 'capital',\n",
       " 'population',\n",
       " 'id']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(WC_DF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to rows of a `DataFrame` either by the expression `DF.loc[label]`, where `label` is the index label of the row we want, or by `DF.iloc[n]`,\n",
    "where `n` is an `int` giving the position of the row in the `DataFrame`.\n",
    "In the case of `WC_DF`, the labels are integers, so we would get the same result using either. You could test this. You could also see the difference if you try finding a row of `top_5_cities` `DataFrame` defined above, after its index labels have been replaced by letters. In this case you could access rows either using letters, using `loc`, or by `int`s, using `iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city               New York\n",
       "city_ascii         New York\n",
       "lat                 40.6943\n",
       "lng                -73.9249\n",
       "country       United States\n",
       "iso2                     US\n",
       "iso3                    USA\n",
       "admin_name         New York\n",
       "capital                 NaN\n",
       "population       19354922.0\n",
       "id               1840034016\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC_DF.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterrating through the rows of a DataFrame\n",
    "A convenient way of going through the rows of a `DataFrame` to perform some operation i by using the `iterrows` method in a `for` loop. This enables you to get both the index label and the row itself, for each successive row of the `DataFrame`. The following code is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tokyo 35.685 139.7514\n",
      "1 New York 40.6943 -73.9249\n",
      "2 Mexico City 19.4424 -99.131\n",
      "3 Mumbai 19.017 72.857\n",
      "4 Sao Paulo -23.5587 -46.625\n"
     ]
    }
   ],
   "source": [
    "for i, row in WC_DF.iterrows():\n",
    "    print(i, row['city_ascii'], row['lat'], row['lng'])\n",
    "    if i >3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the rows of a DataFrame\n",
    "\n",
    "It is easy, and often very useful, to sort the DataFrame by column values using ```.sort_values```, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>Karukh</td>\n",
       "      <td>Karukh</td>\n",
       "      <td>34.4868</td>\n",
       "      <td>62.5918</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Herāt</td>\n",
       "      <td>minor</td>\n",
       "      <td>17484.0</td>\n",
       "      <td>1004546127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>Kōṯah-ye ‘As̲h̲rō</td>\n",
       "      <td>Kotah-ye `Ashro</td>\n",
       "      <td>34.4500</td>\n",
       "      <td>68.8000</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Wardak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35008.0</td>\n",
       "      <td>1004450357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>Shibirghān</td>\n",
       "      <td>Shibirghan</td>\n",
       "      <td>36.6580</td>\n",
       "      <td>65.7383</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Jowzjān</td>\n",
       "      <td>admin</td>\n",
       "      <td>93241.0</td>\n",
       "      <td>1004805783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>Khōst</td>\n",
       "      <td>Khost</td>\n",
       "      <td>33.3395</td>\n",
       "      <td>69.9204</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Khōst</td>\n",
       "      <td>admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004919977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>Maḩmūd-e Rāqī</td>\n",
       "      <td>Mahmud-e Raqi</td>\n",
       "      <td>35.0167</td>\n",
       "      <td>69.3333</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Kāpīsā</td>\n",
       "      <td>admin</td>\n",
       "      <td>7407.0</td>\n",
       "      <td>1004151943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>Lashkar Gāh</td>\n",
       "      <td>Lashkar Gah</td>\n",
       "      <td>31.5830</td>\n",
       "      <td>64.3600</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Helmand</td>\n",
       "      <td>admin</td>\n",
       "      <td>201546.0</td>\n",
       "      <td>1004765445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>Gardēz</td>\n",
       "      <td>Gardez</td>\n",
       "      <td>33.6001</td>\n",
       "      <td>69.2146</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Paktiyā</td>\n",
       "      <td>admin</td>\n",
       "      <td>103601.0</td>\n",
       "      <td>1004468894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>Maīdān Shahr</td>\n",
       "      <td>Maidan Shahr</td>\n",
       "      <td>34.3956</td>\n",
       "      <td>68.8662</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Wardak</td>\n",
       "      <td>admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004798735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>Maīmanah</td>\n",
       "      <td>Maimanah</td>\n",
       "      <td>35.9302</td>\n",
       "      <td>64.7701</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Fāryāb</td>\n",
       "      <td>admin</td>\n",
       "      <td>199795.0</td>\n",
       "      <td>1004622920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399</th>\n",
       "      <td>Andkhōy</td>\n",
       "      <td>Andkhoy</td>\n",
       "      <td>36.9317</td>\n",
       "      <td>65.1015</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Fāryāb</td>\n",
       "      <td>minor</td>\n",
       "      <td>71730.0</td>\n",
       "      <td>1004472345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    city       city_ascii      lat      lng      country iso2  \\\n",
       "10235             Karukh           Karukh  34.4868  62.5918  Afghanistan   AF   \n",
       "8527   Kōṯah-ye ‘As̲h̲rō  Kotah-ye `Ashro  34.4500  68.8000  Afghanistan   AF   \n",
       "3341          Shibirghān       Shibirghan  36.6580  65.7383  Afghanistan   AF   \n",
       "6050               Khōst            Khost  33.3395  69.9204  Afghanistan   AF   \n",
       "5141       Maḩmūd-e Rāqī    Mahmud-e Raqi  35.0167  69.3333  Afghanistan   AF   \n",
       "2088         Lashkar Gāh      Lashkar Gah  31.5830  64.3600  Afghanistan   AF   \n",
       "3210              Gardēz           Gardez  33.6001  69.2146  Afghanistan   AF   \n",
       "6249        Maīdān Shahr     Maidan Shahr  34.3956  68.8662  Afghanistan   AF   \n",
       "2105            Maīmanah         Maimanah  35.9302  64.7701  Afghanistan   AF   \n",
       "7399             Andkhōy          Andkhoy  36.9317  65.1015  Afghanistan   AF   \n",
       "\n",
       "      iso3 admin_name capital  population          id  \n",
       "10235  AFG      Herāt   minor     17484.0  1004546127  \n",
       "8527   AFG     Wardak     NaN     35008.0  1004450357  \n",
       "3341   AFG    Jowzjān   admin     93241.0  1004805783  \n",
       "6050   AFG      Khōst   admin         NaN  1004919977  \n",
       "5141   AFG     Kāpīsā   admin      7407.0  1004151943  \n",
       "2088   AFG    Helmand   admin    201546.0  1004765445  \n",
       "3210   AFG    Paktiyā   admin    103601.0  1004468894  \n",
       "6249   AFG     Wardak   admin         NaN  1004798735  \n",
       "2105   AFG     Fāryāb   admin    199795.0  1004622920  \n",
       "7399   AFG     Fāryāb   minor     71730.0  1004472345  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC_DF.sort_values(by=[\"country\"], ascending=True)[:10] # Sorts countries by alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on __encodings__ of the city name\n",
    "there are two columns that hold the city name. The first column name is `'city'` and\n",
    "the second is `city_ascii`. There are various different ways in which textual\n",
    "information can be encoded into bytes. These days [Unicode characters](https://home.unicode.org/)\n",
    "encoded using [UTF-8](https://en.wikipedia.org/wiki/UTF-8) are pretty standard.\n",
    "But the older [ASCII](https://en.wikipedia.org/wiki/ASCII) code, which uses\n",
    "a single byte per character is still commonly used. Unicode provides a huge\n",
    "variaty of text characters and other symbols, whereas ASCII is quite \n",
    "limited (mainly to characters and symbols found in standard English). \n",
    "But ASCII and is simpler and in \n",
    "some ways easier to deal with than UTF-8. In the following questions you\n",
    "will be asked to use the ASCII version of the city name (from the `city_ascii` column). This mainly just to make you aware that there are different encodings\n",
    "of text strings, but it will also prevent cerain problems that could occur in the\n",
    "Autograder, if different people used different encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering DataFrames\n",
    "By _filtering_ we mean keeping some parts that we want and throwing away others.\n",
    "Typically, we look for rows that match some condition; and the filter condition\n",
    "is often some constraint involving the values for that row in one or more\n",
    "columns.\n",
    "`pandas` `DataFrame`s can  be filtered according values of a column by using a boolean expression, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>19.0170</td>\n",
       "      <td>72.8570</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mahārāshtra</td>\n",
       "      <td>admin</td>\n",
       "      <td>18978000.0</td>\n",
       "      <td>1356226629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>São Paulo</td>\n",
       "      <td>Sao Paulo</td>\n",
       "      <td>-23.5587</td>\n",
       "      <td>-46.6250</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BR</td>\n",
       "      <td>BRA</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>admin</td>\n",
       "      <td>18845000.0</td>\n",
       "      <td>1076532519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>28.6700</td>\n",
       "      <td>77.2300</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>admin</td>\n",
       "      <td>15926000.0</td>\n",
       "      <td>1356872604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>31.2165</td>\n",
       "      <td>121.4365</td>\n",
       "      <td>China</td>\n",
       "      <td>CN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>admin</td>\n",
       "      <td>14987000.0</td>\n",
       "      <td>1156073548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>22.4950</td>\n",
       "      <td>88.3247</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>admin</td>\n",
       "      <td>14787000.0</td>\n",
       "      <td>1356060520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city city_ascii      lat       lng country iso2 iso3   admin_name  \\\n",
       "3     Mumbai     Mumbai  19.0170   72.8570   India   IN  IND  Mahārāshtra   \n",
       "4  São Paulo  Sao Paulo -23.5587  -46.6250  Brazil   BR  BRA    São Paulo   \n",
       "5      Delhi      Delhi  28.6700   77.2300   India   IN  IND        Delhi   \n",
       "6   Shanghai   Shanghai  31.2165  121.4365   China   CN  CHN     Shanghai   \n",
       "7    Kolkata    Kolkata  22.4950   88.3247   India   IN  IND  West Bengal   \n",
       "\n",
       "  capital  population          id  \n",
       "3   admin  18978000.0  1356226629  \n",
       "4   admin  18845000.0  1076532519  \n",
       "5   admin  15926000.0  1356872604  \n",
       "6   admin  14987000.0  1156073548  \n",
       "7   admin  14787000.0  1356060520  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_DF = WC_DF[ WC_DF['capital'] == 'admin'] # This keeps only administrative capitals\n",
    "filtered_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of filtering is a very powerful and useful aspect of `DataFrames`. \n",
    "However, the\n",
    "syntax of the filter operation is rather unusual and a bit difficult to understand.\n",
    "\n",
    "What is happening can be explained by these steps in the way a filter expression\n",
    "is evaluated:\n",
    "*  `DF['label']` (where `DF` is any `DataFrame`), gives a `Series` corresponding\n",
    "    to    the `'label'` column of `DF.\n",
    "    \n",
    "*  `a_series == val` is a special use of `==`. When a Boolean operator\n",
    "    (such `==`, `<` etc.) is applied to a `Series` object\n",
    "   the result is actually a `Series` of Boolean values (not a single Boolean).\n",
    "   The new `Series` obtained will have the value `True` for each element where\n",
    "   the original `Series` satisfies `element == val`, and `False` for the rest.\n",
    "   \n",
    "* `DF[ bool_series ]`, is a special kind of slice-like operation, where a boolean\n",
    "   series is given as a selection argument to the `DataFrame`. It will return \n",
    "   a new DF, containing all the rows of `DF` for which `bool_series` has the \n",
    "   value `True`. (These rows can be quickly found because the `DataFrame` and\n",
    "   the Boolean series both have the same `Index`.)\n",
    "   \n",
    "You do not necessarily need to follow all of that precise desciption of filtering\n",
    "but it will be extremely helpful if you are able to construct filtering\n",
    "operations similar to the above example. You will see another example below,\n",
    "in relation to the earthquake data you will be processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Question 1 tasks\n",
    "\n",
    "This question requires you to write functions to carry out the following specific tasks. \n",
    "\n",
    "* __Question 1a__ --- Find all city names that include *non-ASCII* characters. __[1 Mark]__\n",
    "\n",
    "* __Question 1b__ --- Find city names that occur multiple times in the dataset. __[2 Marks]__\n",
    "* __Question 1c__ --- Create a dictionary of the number of cities in each country that are included in the dataset. __[1 Mark]__\n",
    "\n",
    "* __Question 1d__ --- Create a `DataFrame` of the largest cities (by population) in the world. __[2 Marks]__\n",
    "* __Quesiton 1e__  --- Find all cities in a given country whose population is above a given number. __[2 Marks]__\n",
    "\n",
    "* __Question 1f__ --- Find the total population of people living in cities in a given country. __[2 Marks]__\n",
    "\n",
    "Full details for each task are explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a\n",
    "\n",
    "As we have seen, the names of some cities include accents or symbols that are not represented in the standard ASCII character set. Write a function `non_ascii_cities`, which returns a `Set` containing all the cities that occur in the `world_cities.csv` dataset whose name **cannot** be properly represented using only ASCII characters. \n",
    "\n",
    "Your function should not have any arguments. You should assume that the global variable WC_DF has already been initialised by running the first code cell in this notebook (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1a answer cell\n",
    "\n",
    "def non_ascii_cities():\n",
    "    non_ascii = []\n",
    "    ASCII = '\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x7f'\n",
    "    for i, row in WC_DF.iterrows():\n",
    "        for ch in row['city'].upper():\n",
    "            if ch not in ASCII:\n",
    "                non_ascii.append(row['city'])\n",
    "                break\n",
    "    return set(non_ascii)\n",
    "\n",
    "    # Modify to return a set of all non-ascii city names in the world_cities data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b\n",
    "\n",
    "One issue that you will discover if you investigate the `worldcities.csv` data is that there are many different cities that have the same name. You may find it interesting to discover which are the most common city names. But for this question you need to write a function `num_cities_occurring_n_times(n)`, such that:\n",
    "    \n",
    "* The argument `n` is an integer;\n",
    "* The function returns an integer (type `int`), which is the number of different city names that occur `n` times in the `worldcities` dataset. \n",
    "* The function should consider the city name to be the value in the `city` column. In other words the name in the form that may contain non-ASCII characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1b answer cell\n",
    "\n",
    "def num_cities_occurring_n_times(n):\n",
    "    anslist = []\n",
    "    for i, row in WC_DF.iterrows():\n",
    "        if list(WC_DF['city']).count(row['city']) == n:\n",
    "            anslist.append(row['city'])\n",
    "    return len(set(anslist))\n",
    "    # Modify to return a value according to the specification given above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1c\n",
    "\n",
    "Write a function that returns a dictionary (a `dict` object), whose keys are all the country name strings that occur in the `worldcities` data and whose values are `int`s giving the number of cities of that country that are included in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1c answer cell\n",
    "\n",
    "def country_num_cities_dict():\n",
    "    Dic = {}\n",
    "    temp = {}\n",
    "    for i, row in WC_DF.iterrows():\n",
    "        if row[\"country\"] not in list(Dic.keys()):\n",
    "            temp = {row[\"country\"] : (list(WC_DF['country']).count(row['country']))}\n",
    "            Dic = {**Dic, **temp}\n",
    "    return Dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1d\n",
    "Write a function `largest_cities_dataframe` that takes an `int` argument `n`  and uses the\n",
    "pandas DataFrame `WC_DF` to return a new `DataFrame` containing `n` rows corresponding to\n",
    "the `n` largest cities in terms of population size, in order of decreasing population size.\n",
    "\n",
    "You should return a dataframe such that it has the same columns as the `WC_DF` and each\n",
    "row has the same values as a corresponding row of `WC_DF`. It does not matter if the row\n",
    "indexes are the same. (They may or may not be the same depending on the specific way\n",
    "that you create the new DataFrame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1d answer cell\n",
    "\n",
    "def largest_cities_dataframe(n):\n",
    "    return WC_DF.sort_values(by=[\"population\"], ascending=False)[:n]\n",
    "    # Modify to return a dataframe of the n cities with the largest population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ In answering __1d__ you may assume that no two cities have exactly the same population, which is almost but not quite certain, when dealing with large numbers like this. But, of course, when dealing with quantites where multiple data records could have the same value, we need to be careful, because this may not be the case.\n",
    "For example, if we are interested in what equipement students own, we might think it would be informative\n",
    "to find 'the top 10 students owning the most laptops'. In this case there could be: 1 student with 3 laptops, 23 students with 2 laptops, 160 with 1 laptop and 3 who do not own a laptop. In such a case it is not meaningful to pick the 'top 10' in terms of laptop ownership. A similar problem could potentiall occur with the earthquake data that we will look at later, because the earthquake magnitudes are only recorded to 1 decimal place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1e\n",
    "Define a function `big_cities_in_country( country, population)` that takes as arguments a string corresponding to the name of a country and an integer, which will referes to a population number.\n",
    "The function should return a `list` of the form \n",
    "<center>\n",
    "    <tt>[(\"city1\", pop1), (\"city2\", pop2),... ]</tt>, \n",
    "</center>\n",
    "\n",
    "where each pair\n",
    "`(\"cityN\", popN)` is a `tuple` consisting of the name, **in ASCII form**, of a city in the given `country`, followed by an `int`, which is the population of that city (according to the worldcities data). The list should include all and only those cities in the country whose population is greater than or equal to the given `popuplation` argument. The list should be ordered so that the `(\"cityN\", popN)` items occur in **increasing** order of the population size `popN`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete question 1e in this cell\n",
    "\n",
    "def big_cities_in_country(country, population): # country is a string argument\n",
    "    df = WC_DF[WC_DF[\"country\"]==country][WC_DF[\"population\"]>=population][[\"city\",\"population\"]].sort_values(by=[\"population\"], ascending=True)\n",
    "    return list(df.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1f\n",
    "Create a function that given a country name, returns an `int` which is the total population of \n",
    "people liveing in all the cities of that country, as given in `WC_DF`. \n",
    "\n",
    "**Hints:** \n",
    "* You can ignore cities for which no population value is recorded in `WC_DF`. You will need to be able to deal with `NaN` values. A distinctive feature of `NaN` values is that they have been defined so that `x != x` has the value `True` if `x` has a `NaN` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1f Answer Code Cell\n",
    "\n",
    "def country_total_cities_population(country):\n",
    "    return (int(sum((WC_DF[WC_DF[\"country\"]==country][WC_DF[\"population\"]==WC_DF[\"population\"]][\"population\"]).values.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Earthquakes - Web Access and Pandas DataFrames\n",
    "\n",
    "In this coursework exercise, you will learn how to download live information\n",
    "from the web and procress it using the Pandas data analysis package for Python.\n",
    "\n",
    "The data we will use as an example is from the \n",
    "[United States Geological Survey (USGS)](https://www.usgs.gov/), which \n",
    "provides a wide range of geographic and geological information and data.\n",
    "We shall be using their data relating to seismological \n",
    "events (i.e. Earthquakes) from around the world, which is published in the \n",
    "form of continually updated CSV files. Information about these feeds can\n",
    "be found [here](https://earthquake.usgs.gov/earthquakes/feed/). The URL for the particular feed we shall be using is given below.\n",
    "\n",
    "Questions Overview\n",
    "\n",
    "* __Q2a__ --- Initialise a Pandas DataFrame by downloading earthquake data from the web. __[1 mark]__\n",
    "* __Q2b__ --- Find earthquakes of a given magnitude or higher.           __[1 mark]__\n",
    "* __Q2c__ --- Make a DataFrame showing the most powerful quakes          __[2 marks]__\n",
    "* __Q2d__ --- Make a DataFrame showing distance of quakes from a given location __[3 marks]__\n",
    "* __Q2e__ --- Identify all cities endangered by earthquakes   __[3 marks]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a: Read in data file\n",
    "Read earthquake data from the USGS live feed CSV ```all_day.csv``` into a Pandas DataFrame.\n",
    "The data can be obtained directly from  http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.csv and read into a Pandas DataFrame.\n",
    "\n",
    "__Note:__ For this question you do not need to download and save the file ```all_day.csv```. It should\n",
    "be loaded directly from the web feed. However, while testing, if you have no internet connection or\n",
    "a bad connection you could download a copy of the file. But remember to put it back to downloading\n",
    "the current one before you submit. Note also that ```all_day.csv``` is a live file, which lists\n",
    "quakes recorded during the past 24 hours, and is updated every minute, so of course,\n",
    "you will not always get the same file or the same results. More information about this and other\n",
    "earthquake feeds provided by USGS can be found [here](https://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2a answer code cell\n",
    "\n",
    "import pandas    ## This is the module for creating and manupulating DataFrames\n",
    "\n",
    "# Here we have assigned the url of the quake datasource to the global variable \n",
    "# 'QUAKE_SOURCE' for your convenience.\n",
    "QUAKE_SOURCE = ( \"http://earthquake.usgs.gov/\" +\n",
    "                 \"earthquakes/feed/v1.0/summary/all_day.csv\" )\n",
    "\n",
    "\n",
    "QUAKE_DF = pandas.read_csv(QUAKE_SOURCE)  ## Modify this line to import the data using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use the following cell to test if you have read the quake data into `QUAKE_DF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>...</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-08T19:52:45.090Z</td>\n",
       "      <td>19.1705</td>\n",
       "      <td>-155.496170</td>\n",
       "      <td>34.119999</td>\n",
       "      <td>1.80</td>\n",
       "      <td>md</td>\n",
       "      <td>27.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T19:55:55.830Z</td>\n",
       "      <td>4 km SSW of Pāhala, Hawaii</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>hv</td>\n",
       "      <td>hv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-08T19:45:05.907Z</td>\n",
       "      <td>61.9622</td>\n",
       "      <td>-150.853900</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>ml</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T19:47:21.629Z</td>\n",
       "      <td>28 km E of Skwentna, Alaska</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automatic</td>\n",
       "      <td>ak</td>\n",
       "      <td>ak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-08T19:41:01.040Z</td>\n",
       "      <td>36.1605</td>\n",
       "      <td>-120.309166</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>md</td>\n",
       "      <td>8.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.14490</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T19:42:38.585Z</td>\n",
       "      <td>5 km ENE of Coalinga, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-08T19:37:59.460Z</td>\n",
       "      <td>38.8480</td>\n",
       "      <td>-122.762337</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>1.16</td>\n",
       "      <td>md</td>\n",
       "      <td>7.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.01223</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T19:49:10.918Z</td>\n",
       "      <td>4 km NW of Cobb, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-08T19:33:31.429Z</td>\n",
       "      <td>31.6480</td>\n",
       "      <td>-104.015000</td>\n",
       "      <td>4.705400</td>\n",
       "      <td>1.90</td>\n",
       "      <td>ml(texnet)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T19:51:41.149Z</td>\n",
       "      <td>39 km W of Mentone, Texas</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.328851</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>tx</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  latitude   longitude      depth   mag  \\\n",
       "0  2023-11-08T19:52:45.090Z   19.1705 -155.496170  34.119999  1.80   \n",
       "1  2023-11-08T19:45:05.907Z   61.9622 -150.853900  67.500000  2.00   \n",
       "2  2023-11-08T19:41:01.040Z   36.1605 -120.309166  10.200000  0.98   \n",
       "3  2023-11-08T19:37:59.460Z   38.8480 -122.762337   0.940000  1.16   \n",
       "4  2023-11-08T19:33:31.429Z   31.6480 -104.015000   4.705400  1.90   \n",
       "\n",
       "      magType   nst    gap     dmin   rms  ...                   updated  \\\n",
       "0          md  27.0  209.0      NaN  0.09  ...  2023-11-08T19:55:55.830Z   \n",
       "1          ml   NaN    NaN      NaN  0.31  ...  2023-11-08T19:47:21.629Z   \n",
       "2          md   8.0  190.0  0.14490  0.02  ...  2023-11-08T19:42:38.585Z   \n",
       "3          md   7.0  193.0  0.01223  0.03  ...  2023-11-08T19:49:10.918Z   \n",
       "4  ml(texnet)  16.0   44.0  0.00000  0.10  ...  2023-11-08T19:51:41.149Z   \n",
       "\n",
       "                         place        type horizontalError depthError  \\\n",
       "0   4 km SSW of Pāhala, Hawaii  earthquake            0.78   1.000000   \n",
       "1  28 km E of Skwentna, Alaska  earthquake             NaN   0.700000   \n",
       "2     5 km ENE of Coalinga, CA  earthquake            1.01   2.100000   \n",
       "3          4 km NW of Cobb, CA  earthquake            0.61   0.740000   \n",
       "4    39 km W of Mentone, Texas  earthquake            0.00   1.328851   \n",
       "\n",
       "   magError  magNst     status  locationSource magSource  \n",
       "0      0.59     3.0  automatic              hv        hv  \n",
       "1       NaN     NaN  automatic              ak        ak  \n",
       "2      0.04     2.0  automatic              nc        nc  \n",
       "3      0.14     9.0  automatic              nc        nc  \n",
       "4      0.20    13.0  automatic              tx        tx  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## If QUAKE_DF is a DataFrame, show the first 5 rows\n",
    "try:\n",
    "    if type(QUAKE_DF) == pandas.DataFrame:\n",
    "        display(QUAKE_DF.head())\n",
    "    else:\n",
    "        print(\"QUAKE_DF is not a DataFrame\")\n",
    "except:\n",
    "    print(\"QUAKE_DF has not been assigned a value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "The columns containing latitude and longitude values are labelled differently in the `worldcities.csv` and the earthquake data from USGS. This is a minor but very typical form of incompatibility between data formats that you will often need to deal with when working with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples of useful `pandas` functions\n",
    "\n",
    "Here we show you some more pandas functions that you may find useful in this exercise. \n",
    "\n",
    "As we have seen, versatile filtering and sorting capabilities are provided by pandas. To get more understanding of these, you should look at tutorials of using Pandas DataFrames. But the following example illustrates how you can find and display quakes whose depth is greater than or equal to a given threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_deep_quakes( depth ):\n",
    "    # make deep_quakes DataFrame by selecting rows from QUAKE_DF\n",
    "    deep_quakes = QUAKE_DF[ QUAKE_DF[\"depth\"] >= depth ]  ## This is how you select rows by a condition\n",
    "                                                          ## on one of the column values.\n",
    "        \n",
    "    print(\"Number of quakes of depth {} or deeper:\".format(depth), \n",
    "           len(deep_quakes.index))     ## This finds the number of rows of the deep_quakes DataFrame\n",
    "    \n",
    "    display(deep_quakes.sort_values(\"depth\", ascending=False))  ## Sort by descending depth value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "The `QUAKES_DF` global variable needs to be set before these examples will work, so I am using a `try`, `except` construct to avoid getting an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of quakes of depth 100 or deeper: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>...</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2023-11-08T03:44:27.227Z</td>\n",
       "      <td>-23.3322</td>\n",
       "      <td>-179.5710</td>\n",
       "      <td>550.924</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>4.582</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T04:03:55.040Z</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>14.26</td>\n",
       "      <td>12.428</td>\n",
       "      <td>0.113</td>\n",
       "      <td>23.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2023-11-08T12:28:16.882Z</td>\n",
       "      <td>-23.9740</td>\n",
       "      <td>-179.6124</td>\n",
       "      <td>477.355</td>\n",
       "      <td>4.5</td>\n",
       "      <td>mb</td>\n",
       "      <td>43.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.939</td>\n",
       "      <td>1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T12:42:10.040Z</td>\n",
       "      <td>south of the Fiji Islands</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>12.30</td>\n",
       "      <td>9.338</td>\n",
       "      <td>0.086</td>\n",
       "      <td>39.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2023-11-08T13:56:20.826Z</td>\n",
       "      <td>21.5708</td>\n",
       "      <td>143.0808</td>\n",
       "      <td>320.395</td>\n",
       "      <td>4.6</td>\n",
       "      <td>mb</td>\n",
       "      <td>157.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.119</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T14:51:36.040Z</td>\n",
       "      <td>Mariana Islands region</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>7.85</td>\n",
       "      <td>5.099</td>\n",
       "      <td>0.035</td>\n",
       "      <td>248.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2023-11-08T10:02:02.233Z</td>\n",
       "      <td>63.0999</td>\n",
       "      <td>-150.8850</td>\n",
       "      <td>131.100</td>\n",
       "      <td>2.4</td>\n",
       "      <td>ml</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T10:17:31.040Z</td>\n",
       "      <td>64 km SE of Denali National Park, Alaska</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automatic</td>\n",
       "      <td>ak</td>\n",
       "      <td>ak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2023-11-08T04:13:01.334Z</td>\n",
       "      <td>60.0810</td>\n",
       "      <td>-153.0992</td>\n",
       "      <td>113.900</td>\n",
       "      <td>2.6</td>\n",
       "      <td>ml</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T04:15:22.687Z</td>\n",
       "      <td>65 km ENE of Pedro Bay, Alaska</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>automatic</td>\n",
       "      <td>ak</td>\n",
       "      <td>ak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2023-11-08T10:59:43.504Z</td>\n",
       "      <td>-22.0908</td>\n",
       "      <td>-68.8081</td>\n",
       "      <td>111.064</td>\n",
       "      <td>4.4</td>\n",
       "      <td>mwr</td>\n",
       "      <td>17.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T11:10:03.040Z</td>\n",
       "      <td>42 km NNE of Calama, Chile</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>5.62</td>\n",
       "      <td>8.695</td>\n",
       "      <td>0.080</td>\n",
       "      <td>15.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2023-11-08T12:39:41.924Z</td>\n",
       "      <td>37.0299</td>\n",
       "      <td>30.2316</td>\n",
       "      <td>100.993</td>\n",
       "      <td>4.4</td>\n",
       "      <td>mb</td>\n",
       "      <td>68.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-08T13:05:59.040Z</td>\n",
       "      <td>Western Turkey</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.924</td>\n",
       "      <td>0.103</td>\n",
       "      <td>27.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         time  latitude  longitude    depth  mag magType  \\\n",
       "197  2023-11-08T03:44:27.227Z  -23.3322  -179.5710  550.924  4.5      mb   \n",
       "94   2023-11-08T12:28:16.882Z  -23.9740  -179.6124  477.355  4.5      mb   \n",
       "65   2023-11-08T13:56:20.826Z   21.5708   143.0808  320.395  4.6      mb   \n",
       "151  2023-11-08T10:02:02.233Z   63.0999  -150.8850  131.100  2.4      ml   \n",
       "193  2023-11-08T04:13:01.334Z   60.0810  -153.0992  113.900  2.6      ml   \n",
       "127  2023-11-08T10:59:43.504Z  -22.0908   -68.8081  111.064  4.4     mwr   \n",
       "88   2023-11-08T12:39:41.924Z   37.0299    30.2316  100.993  4.4      mb   \n",
       "\n",
       "       nst    gap   dmin   rms  ...                   updated  \\\n",
       "197   28.0  103.0  4.582  0.52  ...  2023-11-08T04:03:55.040Z   \n",
       "94    43.0  100.0  4.939  1.10  ...  2023-11-08T12:42:10.040Z   \n",
       "65   157.0   38.0  8.119  0.59  ...  2023-11-08T14:51:36.040Z   \n",
       "151    NaN    NaN    NaN  0.30  ...  2023-11-08T10:17:31.040Z   \n",
       "193    NaN    NaN    NaN  0.75  ...  2023-11-08T04:15:22.687Z   \n",
       "127   17.0   46.0  0.878  0.70  ...  2023-11-08T11:10:03.040Z   \n",
       "88    68.0   48.0  0.382  0.86  ...  2023-11-08T13:05:59.040Z   \n",
       "\n",
       "                                        place        type horizontalError  \\\n",
       "197                 south of the Fiji Islands  earthquake           14.26   \n",
       "94                  south of the Fiji Islands  earthquake           12.30   \n",
       "65                     Mariana Islands region  earthquake            7.85   \n",
       "151  64 km SE of Denali National Park, Alaska  earthquake             NaN   \n",
       "193            65 km ENE of Pedro Bay, Alaska  earthquake             NaN   \n",
       "127                42 km NNE of Calama, Chile  earthquake            5.62   \n",
       "88                             Western Turkey  earthquake            6.09   \n",
       "\n",
       "    depthError  magError  magNst     status  locationSource magSource  \n",
       "197     12.428     0.113    23.0   reviewed              us        us  \n",
       "94       9.338     0.086    39.0   reviewed              us        us  \n",
       "65       5.099     0.035   248.0   reviewed              us        us  \n",
       "151      0.700       NaN     NaN  automatic              ak        ak  \n",
       "193      1.700       NaN     NaN  automatic              ak        ak  \n",
       "127      8.695     0.080    15.0   reviewed              us        us  \n",
       "88       6.924     0.103    27.0   reviewed              us        us  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    show_deep_quakes(100)\n",
    "except:\n",
    "    print(\"Probably QUAKE_DF not correctly set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find ```max``` and ```min``` values in a column. Eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    QUAKE_DF[\"depth\"].max()\n",
    "except:\n",
    "    print(\"Probably QUAKE_DF not correctly set\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    QUAKE_DF[\"depth\"].min()\n",
    "except:\n",
    "    print(\"Probably QUAKE_DF not correctly set\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b: Find Powerful Quakes\n",
    "\n",
    "Write a function `powerful_quakes` that takes a numerical argument and returns a `DataFrame` including\n",
    "all the quakes in `QUAKE_DF` that have a magnitude greater than or equal to the given argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete question 2b answer cell\n",
    "\n",
    "def powerful_quakes(mag):\n",
    "    return QUAKE_DF[QUAKE_DF[\"mag\"]>=mag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c: Find `n+` most powerful earthquakes\n",
    "\n",
    "Produce a `DataFrame` with rows represent the `n`(or maybe more) most powerful quakes\n",
    "in descending order of magnitude. The returned ``DataFrame`` should show at least `n`\n",
    "quakes and may sometimes show more since we do not want to leave out any quake that is equally\n",
    "powerful as the last quake listed in the `DataFrame`,\n",
    "More specificially, we want the function to return a `DataFrame` that:\n",
    "* has exactly the same column names as `QUAKES_DF`,\n",
    "* each row has the same vaues in each column as a corresponding row in `QUAKES_DF` (but it does not matter whether the row indices in the returned `DataFrame` are the same or different from corresponding rows in `QUAKES_DF`),\n",
    "* the rows of the returned `DataFrame` are ordered in _descending order_ of their magnitude column value (rows of equal magnitude can appear in any order),\n",
    "* contains all and only those rows `QUAKES_DF`, such that there are fewer than `n` other rows in\n",
    "  `QUAKES_DF` that have a higher magnitude.\n",
    "\n",
    "\n",
    "#### Note:\n",
    "The above definition of the requirements is clear and precise. Though you may ask for help and advice regarding implementation, you will not be given help with understanding the specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2c answer cell\n",
    "\n",
    "def most_powerful_n_quakes(n):\n",
    "    df = QUAKE_DF.sort_values([\"mag\"],ascending=False)\n",
    "    c=0\n",
    "    for i,row in df.iterrows():\n",
    "        if(c==n-1):\n",
    "            return df[df[\"mag\"]>=int(row[[\"mag\"]])]\n",
    "        c = c+1\n",
    "    # Edit this function to make it return a DataFrame of \n",
    "    # the 'top n' quakes of the all_day.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between locations on the Earth's surface\n",
    "\n",
    "Clearly, when dealing with data pertaining to locations in space, the distance between such locations is often of great significance when interpreting or extracting further information from the data.\n",
    "\n",
    "To help answer the following questions you are provided with the function ```haversine_distance```, \n",
    "which implements the [_Haversine formula_](https://en.wikipedia.org/wiki/Haversine_formula) to find the surface distance in kilometres between two locations, that are specified in terms of\n",
    "latitude and longitude values. When finding distances betwen points on the surface of the\n",
    "Earth we need to use this formula, rather than the simpler Pythagorean distance formula,\n",
    "because the Earth's surface is a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to compute distance between locations (kilometres) \n",
    "# Returns the surface distance in meters, according to the Haversine formula,\n",
    "# between two locations given as (latitude, longitude) coordinate pairs.\n",
    "\n",
    "import math\n",
    "def haversine_distance( loc1 , loc2 ): \n",
    "    '''finds the distance (m) between 2 locations, where locations are defined by\n",
    "    longitudes and latitudes'''\n",
    "    lat1, lon1 = loc1\n",
    "    lat2, lon2 = loc2\n",
    "    radius = 6371  # kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = (math.sin(dlat / 2) * math.sin(dlat / 2) +\n",
    "         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "         math.sin(dlon / 2) * math.sin(dlon / 2))\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2d: Sort quakes by distance from a given location\n",
    "\n",
    "Write a function `quake_distance_from_loc_dataframe(loc)` satisfying the following requirements:\n",
    "* It takes a location argument `(latitude,longitude)` consisting of a pair of `float`s \n",
    "    (Note: this is a single argument but consists of a pair of values represented as Python `tuple`.)\n",
    "\n",
    "* It returns a `DataFrame` object derived from `QUAKE_DF` but with one extra column `distance_from_loc`, giving the distance of each quake from the given location. \n",
    "\n",
    "* The rows of the returned `DataFrame` should be the same as those in `QUAKE_DF` except for the aditional `distance_from_loc` column. (However, it is not necessary to preserve the index of the `DataFrame`, this will be ignored when your solution is tested.)\n",
    "\n",
    "* The rows of the returned `DataFrame` should be _sorted_ in order of _increasing_ values of `distance_from_loc`. \n",
    "\n",
    "* The original DataFrame `QUAKE_DF` should not be altered by the execution of \n",
    "`quake_distance_from_loc_dataframe(loc)`.\n",
    "\n",
    "#### Note:\n",
    "You will need to do some research to find out how to create a new column and set its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2d Answer Code Cell\n",
    "\n",
    "def quake_distance_from_loc_dataframe(loc):\n",
    "    column_values = []\n",
    "    df = QUAKE_DF.copy()\n",
    "    for i,row in QUAKE_DF.iterrows():\n",
    "        column_values.append(haversine_distance((row[\"latitude\"],row[\"longitude\"]),loc))\n",
    "    df[\"distance_from_loc\"]=column_values\n",
    "    df = df.sort_values(by=[\"distance_from_loc\"], ascending=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2e: Identifying Endangered Cities\n",
    "\n",
    "The idea of this question is to identify possible emergency situations by finding\n",
    "cities that are likely to suffer from the effects of\n",
    "an earthquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of an qarthquake at a distance from its epicenter\n",
    "The effect of an earthquake on a city or person will depend on their distance from the source of the quake. The effect of an earthquake will depend on many factors and even the dependence on distance to source is very complex. However, after a bit of background research, Brandon has come up with a simple formula which hopefully at least gives a very crude estimate of relative effect of a quake with a particular magnitude and depth on a surface location at a known surface distance from the quake's epicenter. The calculated effective magnitude of an earthquake will be less that the source magnitude, for instance a magnitude 9 quake at a depth of 100km (which is likely to be extremely destructive), would have an effective magnitude of 5 at its epicentre (directly above the source) and 3.585 at a point on the earth surface 500km away from the epicenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_magnitude( magnitude, depth, surface_distance ):\n",
    "    energy = 10**magnitude  # convert logarithmic magnitude to a linear energy value\n",
    "    if depth < 1:   # Crude fix for small or negative depths (can occur where land is above sea level)\n",
    "        depth = 1\n",
    "    ## Calculate distance to source by Pythagorus (ignoring curvature of surface)\n",
    "    dist_to_source_squared =  depth**2 + surface_distance**2\n",
    "    ## Apply inverse square distance multiplier to get energy density at distance from source\n",
    "    ## (Ignores damping effects)\n",
    "    attenuated_energy = energy/dist_to_source_squared\n",
    "    attenuated_magnitude =  math.log10(attenuated_energy) ## Convert back to a log base 10 scale\n",
    "    return attenuated_magnitude\n",
    "\n",
    "# Some test cases.\n",
    "#effective_magnitude(9,100,500)\n",
    "#effective_magnitude(6,50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _epicenter_ of an earthquake is the point on the earth that is directly above its source. Thus the effective magnitude at the epicenter is just the effective magnitude of the quake at surface distance zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epicenter_magnitude( magnitude, depth ):\n",
    "    return effective_magnitude( magnitude, depth, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For any given quake, its `effective_magnitude` at any point on Earth is always less than or equal to its `epicenter_magnitude`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of the `endangered_cities` function\n",
    "\n",
    "Now we get to the specification of the function.\n",
    "\n",
    "Write a function `endangered_cities( min_population, min_effective_magnitude)`\n",
    "that takes \n",
    "two numerical arguments: an `int` (`min_population`) and a `float` (`min_effective_magnitude`) \n",
    "and  returns a `list` specifying all those cities listed in \n",
    "the `WC_DF` such that:\n",
    "* the city has  a population greater than or equal to the given `min_population`;\n",
    "\n",
    "* for at least one of the quakes recorded in `QUAKE_DF`, the `effective_magnitude` (as determined by the\n",
    "  function defined above) of that quake\n",
    "   at the location of the city (as given in `worldcities.csv`) is equal to or greater than the  `min_effective_magnitude`.\n",
    "   \n",
    "* each city in the list should be represented by a tuple `(city_ascii, country, (lat, lng))`, giving\n",
    "the city name in ASCII form, the country and the location (as a latitude, longitude pair).\n",
    "\n",
    "* the returned list should be ordered aphabetically, primarily in terms of `country` and secondarily\n",
    "  cities of the same country should be ordered in terms of `city_ascii`. \n",
    "  This ordering is illustrated by the sample output given below.\n",
    "\n",
    "* A final condition is that the function should run in a reasonable time frame such that it can be expected to give correct and complete results after no more that 5 minutes excecution time. To ensure that this is feasible, the example cases you will be tested on, will be ones for which Brandon's solution ran in less than 2 minues on the Gradescope autograder platform. (All submissions will be tested on the same saved copy of `all_day.csv` previously downloaded from the USGS feed.)\n",
    "   \n",
    "##### Example Output:   \n",
    "\n",
    "<pre>\n",
    " in [165]    %time        \n",
    "             endangered_cities(200000, 0.5)\n",
    " \n",
    " out[165]    CPU times: user 1min 58s, sys: 768 µs, total: 1min 58s\n",
    "             Wall time: 1min 58s\n",
    "             [('Baghlan', 'Afghanistan', (36.1393, 68.6993)),\n",
    "              ('Kunduz', 'Afghanistan', (36.728, 68.8725)),\n",
    "              ('Mazar-e Sharif', 'Afghanistan', (36.7, 67.1)),\n",
    "              ('Ambon', 'Indonesia', (-3.7167, 128.2)),\n",
    "              ('Denov', 'Uzbekistan', (38.2772, 67.8872))]\n",
    "</pre>\n",
    "\n",
    "##### Notes:\n",
    "\n",
    "* Your code should make use of the functions defined above: `haversine_distance`, `effective_magnitude` and `epicenter_magnitude`. You are advised not to change these otherwise you may get different results from what the Autograder is expecting.\n",
    "\n",
    "* The calculation of this function is computationally intensive. For prelimiary testing you might use smaller data sets by, say only using cities in one country. \n",
    "\n",
    "* There are many optimisations that can be done to reduce the computational cost of finding the endangered cities. For example, calculating `epicenter_magnitude` enables weaker earthquakes to be ignored without the need to determine their distance from every city on earth. It may also be useful to not that when calcuating the effective magnitude of a given quake (with a specific depth) at different locations, the value always decreases as surface distance from the quake increases. \n",
    "\n",
    "* Remember that the autograder will run your actual code, so will probably take a while to grade this function. However, it only runs the function once and then performs 3 different tests on the value that is returned.\n",
    "\n",
    "* The autograder will run on a presaved set of quakes (so the test is the same for all submissions). However, your code should run on any version of `all_day.csv` that you download from the USGS live feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2e Answer Code Cell\n",
    "def endangered_cities(min_population, min_effective_magnitude):\n",
    "    df1 = WC_DF[WC_DF[\"population\"]>=min_population]\n",
    "    df2 = pandas.DataFrame(columns = [\"latitude\",\"longitude\",\"depth\",\"mag\"])\n",
    "    preans = pandas.DataFrame(columns = [\"city_ascii\",\"country\",\"lat\",\"lng\",\"id\"])\n",
    "    ans = []\n",
    "    c=0\n",
    "    for i,row in QUAKE_DF.iterrows():\n",
    "        if epicenter_magnitude( row[\"mag\"], row[\"depth\"] ) >= min_effective_magnitude:\n",
    "            df2.loc[len(df2)] = row\n",
    "            c = c+1\n",
    "    c=0\n",
    "    for i, row in df2.iterrows():\n",
    "        for j, row2 in df1.iterrows():\n",
    "            if row2[\"id\"] not in preans[\"id\"]:\n",
    "                if effective_magnitude(row[\"mag\"], row[\"depth\"], haversine_distance((row[\"latitude\"],row[\"longitude\"]), (row2[\"lat\"],row2[\"lng\"])))>= min_effective_magnitude:\n",
    "                    preans.loc[len(preans)] = row2[[\"city_ascii\",\"country\",\"lat\",\"lng\",\"id\"]]\n",
    "                    c = c+1\n",
    "    preans[\"comb\"] = preans[\"country\"] +  preans[\"city_ascii\"]\n",
    "    preans = preans.sort_values(by=[\"comb\"], ascending=True)\n",
    "    preans = preans.drop(columns = [\"id\",\"comb\"])\n",
    "    for i, row in preans.iterrows():\n",
    "        ans.append((row[\"city_ascii\"],row[\"country\"],(row[\"lat\"],row[\"lng\"])))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
